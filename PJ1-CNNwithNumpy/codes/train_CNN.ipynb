{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T15:30:08.977753Z",
     "start_time": "2025-04-12T15:30:08.008310Z"
    }
   },
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from struct import unpack\n",
    "import pickle\n",
    "import mynn as nn\n",
    "\n",
    "# 固定随机种子\n",
    "np.random.seed(309)\n",
    "\n",
    "# 数据路径\n",
    "train_images_path = r'.\\dataset\\MNIST\\train-images-idx3-ubyte.gz'\n",
    "train_labels_path = r'.\\dataset\\MNIST\\train-labels-idx1-ubyte.gz'\n",
    "\n",
    "# 加载图像数据\n",
    "with gzip.open(train_images_path, 'rb') as f:\n",
    "    magic, num, rows, cols = unpack('>4I', f.read(16))\n",
    "    train_imgs = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows, cols)  # shape (60000, 28, 28)\n",
    "    train_imgs = train_imgs[:, np.newaxis, :, :]  # 最终shape (60000, 1, 28, 28)\n",
    "\n",
    "# 加载标签数据\n",
    "with gzip.open(train_labels_path, 'rb') as f:\n",
    "    magic, num = unpack('>2I', f.read(8))\n",
    "    train_labs = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "# 数据划分\n",
    "idx = np.random.permutation(np.arange(num))\n",
    "train_imgs = train_imgs[idx]\n",
    "train_labs = train_labs[idx]\n",
    "valid_imgs = train_imgs[:320]\n",
    "valid_labs = train_labs[:320]\n",
    "train_imgs = train_imgs[10000:]\n",
    "train_labs = train_labs[10000:]\n",
    "\n",
    "# 归一化\n",
    "train_imgs = train_imgs / 255.0  \n",
    "valid_imgs = valid_imgs / 255.0\n",
    "\n",
    "\n",
    "print(\"\\n=== CNN数据预处理验证 ===\")\n",
    "print(f\"训练集图像形状: {train_imgs.shape} (样本数, 通道数, 高, 宽)\")\n",
    "print(f\"单个样本形状: {train_imgs[0].shape}\")\n",
    "print(f\"训练集标签形状: {train_labs.shape}\")\n",
    "print(f\"验证集图像形状: {valid_imgs.shape}\")\n",
    "print(f\"像素值范围: [{train_imgs.min()}, {train_imgs.max()}]\")\n",
    "\n",
    "# 可视化第一个样本\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.imshow(train_imgs[0][0], cmap='gray')  \n",
    "plt.title(f\"Label: {train_labs[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CNN数据预处理验证 ===\n",
      "训练集图像形状: (50000, 1, 28, 28) (样本数, 通道数, 高, 宽)\n",
      "单个样本形状: (1, 28, 28)\n",
      "训练集标签形状: (50000,)\n",
      "验证集图像形状: (320, 1, 28, 28)\n",
      "像素值范围: [0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T10:57:58.819652Z",
     "start_time": "2025-04-03T10:36:55.982936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\"\"\" conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2),  # 输出 [batch, 6, 28, 28]\n",
    "            ReLU(),\n",
    "           \n",
    "\n",
    "            Flatten(),  # 输出 [batch, 32*20*20]\n",
    "            Linear(in_dim=6 * 28 * 28, out_dim=10) , # 输出 [batch, 10]\"\"\"\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.01, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[800, 2400, 4000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=5, log_iters=100, save_dir=r'./best_models')"
   ],
   "id": "bf7084b488ddfaa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 16.54037645117124, score: 0.09375\n",
      "[Dev] loss: 16.401676601236286, score: 0.094\n",
      "epoch: 0, iteration: 100\n",
      "[Train] loss: 11.74539197231876, score: 0.25\n",
      "[Dev] loss: 11.309151170173397, score: 0.306\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 7.412734234720032, score: 0.53125\n",
      "[Dev] loss: 8.548289912083607, score: 0.464\n",
      "epoch: 0, iteration: 300\n",
      "[Train] loss: 4.911106699944508, score: 0.65625\n",
      "[Dev] loss: 6.7769397162637866, score: 0.558\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 6.567631604544635, score: 0.59375\n",
      "[Dev] loss: 5.776305730500354, score: 0.612\n",
      "epoch: 0, iteration: 500\n",
      "[Train] loss: 4.275331303346313, score: 0.71875\n",
      "[Dev] loss: 5.069775329829601, score: 0.658\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 3.4972706925845585, score: 0.75\n",
      "[Dev] loss: 4.549535392473795, score: 0.684\n",
      "epoch: 0, iteration: 700\n",
      "[Train] loss: 4.445674055234516, score: 0.71875\n",
      "[Dev] loss: 4.161282687767456, score: 0.7\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 4.2976976465221535, score: 0.6875\n",
      "[Dev] loss: 3.8864452376385867, score: 0.722\n",
      "epoch: 0, iteration: 900\n",
      "[Train] loss: 2.5697323706401187, score: 0.8125\n",
      "[Dev] loss: 3.6682825102369296, score: 0.736\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 3.3728368215218483, score: 0.71875\n",
      "[Dev] loss: 3.6227182756861733, score: 0.743\n",
      "epoch: 0, iteration: 1100\n",
      "[Train] loss: 2.306603991244802, score: 0.78125\n",
      "[Dev] loss: 3.4902243091876004, score: 0.75\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 4.628589564427753, score: 0.65625\n",
      "[Dev] loss: 3.373324056155432, score: 0.755\n",
      "epoch: 0, iteration: 1300\n",
      "[Train] loss: 1.513403964972849, score: 0.875\n",
      "[Dev] loss: 3.340745275259485, score: 0.76\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 5.616394609877073, score: 0.625\n",
      "[Dev] loss: 3.192846822685562, score: 0.765\n",
      "epoch: 0, iteration: 1500\n",
      "[Train] loss: 4.352714766305484, score: 0.6875\n",
      "[Dev] loss: 3.196713112994019, score: 0.769\n",
      "best accuracy performence has been updated: 0.00000 --> 0.76500\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 2.17725375141082, score: 0.78125\n",
      "[Dev] loss: 3.1840620848128003, score: 0.764\n",
      "epoch: 1, iteration: 100\n",
      "[Train] loss: 3.5216255048351592, score: 0.71875\n",
      "[Dev] loss: 3.0463695559295756, score: 0.778\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 2.831374523347158, score: 0.78125\n",
      "[Dev] loss: 3.0099616974719994, score: 0.781\n",
      "epoch: 1, iteration: 300\n",
      "[Train] loss: 3.2360969312001306, score: 0.8125\n",
      "[Dev] loss: 2.920725615581184, score: 0.783\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 3.2579258234157313, score: 0.78125\n",
      "[Dev] loss: 2.9140293385858245, score: 0.786\n",
      "epoch: 1, iteration: 500\n",
      "[Train] loss: 2.875790390232492, score: 0.84375\n",
      "[Dev] loss: 2.817139169928912, score: 0.798\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 3.7187566323164774, score: 0.6875\n",
      "[Dev] loss: 2.7803693810509533, score: 0.792\n",
      "epoch: 1, iteration: 700\n",
      "[Train] loss: 3.681111516022038, score: 0.71875\n",
      "[Dev] loss: 2.721142899316407, score: 0.799\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 2.8585539232165385, score: 0.78125\n",
      "[Dev] loss: 2.6939421357428817, score: 0.8\n",
      "epoch: 1, iteration: 900\n",
      "[Train] loss: 2.4077791063756417, score: 0.8125\n",
      "[Dev] loss: 2.694666970828719, score: 0.795\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 2.3156496599306973, score: 0.875\n",
      "[Dev] loss: 2.6744540471878246, score: 0.799\n",
      "epoch: 1, iteration: 1100\n",
      "[Train] loss: 2.5337329639781343, score: 0.8125\n",
      "[Dev] loss: 2.660319009410895, score: 0.802\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 4.509501966541363, score: 0.6875\n",
      "[Dev] loss: 2.632642132067783, score: 0.802\n",
      "epoch: 1, iteration: 1300\n",
      "[Train] loss: 2.4245888480260067, score: 0.8125\n",
      "[Dev] loss: 2.6140996975933164, score: 0.805\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: 1.37176633128814, score: 0.8125\n",
      "[Dev] loss: 2.6260023582466525, score: 0.802\n",
      "epoch: 1, iteration: 1500\n",
      "[Train] loss: 1.340800258413008, score: 0.90625\n",
      "[Dev] loss: 2.574947928683547, score: 0.808\n",
      "best accuracy performence has been updated: 0.76500 --> 0.80900\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 2.98960226992802, score: 0.78125\n",
      "[Dev] loss: 2.5624006708426585, score: 0.809\n",
      "epoch: 2, iteration: 100\n",
      "[Train] loss: 2.4455067896390608, score: 0.8125\n",
      "[Dev] loss: 2.5597624823405303, score: 0.811\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: 4.695804297518155, score: 0.65625\n",
      "[Dev] loss: 2.5603363495488067, score: 0.807\n",
      "epoch: 2, iteration: 300\n",
      "[Train] loss: 1.0182824307990268, score: 0.875\n",
      "[Dev] loss: 2.5493922341991855, score: 0.81\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 2.1523670096455456, score: 0.8125\n",
      "[Dev] loss: 2.519605065575399, score: 0.813\n",
      "epoch: 2, iteration: 500\n",
      "[Train] loss: 3.2672737057031176, score: 0.71875\n",
      "[Dev] loss: 2.4956124741255405, score: 0.813\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 2.7816109083500216, score: 0.84375\n",
      "[Dev] loss: 2.4721900984148575, score: 0.812\n",
      "epoch: 2, iteration: 700\n",
      "[Train] loss: 2.7125973612788363, score: 0.8125\n",
      "[Dev] loss: 2.4635769093002384, score: 0.814\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 2.5797502943441453, score: 0.75\n",
      "[Dev] loss: 2.462971648422943, score: 0.817\n",
      "epoch: 2, iteration: 900\n",
      "[Train] loss: 3.014691979839124, score: 0.78125\n",
      "[Dev] loss: 2.441366904816402, score: 0.816\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 1.1630104430870873, score: 0.9375\n",
      "[Dev] loss: 2.4339931810545457, score: 0.819\n",
      "epoch: 2, iteration: 1100\n",
      "[Train] loss: 3.9666625590045452, score: 0.71875\n",
      "[Dev] loss: 2.4493961187933935, score: 0.815\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 2.977998590322228, score: 0.78125\n",
      "[Dev] loss: 2.427148806278358, score: 0.818\n",
      "epoch: 2, iteration: 1300\n",
      "[Train] loss: 4.765020105663993, score: 0.6875\n",
      "[Dev] loss: 2.4304734256603284, score: 0.819\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: 2.4906002160983256, score: 0.8125\n",
      "[Dev] loss: 2.4297167069027354, score: 0.82\n",
      "epoch: 2, iteration: 1500\n",
      "[Train] loss: 1.0122553352247723, score: 0.90625\n",
      "[Dev] loss: 2.4146575995185313, score: 0.822\n",
      "best accuracy performence has been updated: 0.80900 --> 0.82100\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 2.346697179794357, score: 0.84375\n",
      "[Dev] loss: 2.401108342281454, score: 0.822\n",
      "epoch: 3, iteration: 100\n",
      "[Train] loss: 1.065423925258882, score: 0.90625\n",
      "[Dev] loss: 2.40280134088346, score: 0.822\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: 2.8430252136500362, score: 0.8125\n",
      "[Dev] loss: 2.3948531647303253, score: 0.82\n",
      "epoch: 3, iteration: 300\n",
      "[Train] loss: 2.1411278874540773, score: 0.84375\n",
      "[Dev] loss: 2.3861555200573252, score: 0.823\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 2.6801846700875034, score: 0.84375\n",
      "[Dev] loss: 2.386769415364831, score: 0.822\n",
      "epoch: 3, iteration: 500\n",
      "[Train] loss: 4.693384317884108, score: 0.71875\n",
      "[Dev] loss: 2.38766309020976, score: 0.823\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 3.458688788117625, score: 0.75\n",
      "[Dev] loss: 2.3881634111582772, score: 0.822\n",
      "epoch: 3, iteration: 700\n",
      "[Train] loss: 2.1640647541576863, score: 0.8125\n",
      "[Dev] loss: 2.3825612070400264, score: 0.824\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 0.002824208802245472, score: 1.0\n",
      "[Dev] loss: 2.3788662743682183, score: 0.826\n",
      "epoch: 3, iteration: 900\n",
      "[Train] loss: 2.029544300144757, score: 0.875\n",
      "[Dev] loss: 2.366473914733964, score: 0.824\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 2.713768885076707, score: 0.84375\n",
      "[Dev] loss: 2.357323416594437, score: 0.825\n",
      "epoch: 3, iteration: 1100\n",
      "[Train] loss: 1.5181984139191544, score: 0.84375\n",
      "[Dev] loss: 2.3474705739688284, score: 0.823\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: 2.577148935165341, score: 0.8125\n",
      "[Dev] loss: 2.3411015563989306, score: 0.825\n",
      "epoch: 3, iteration: 1300\n",
      "[Train] loss: 3.7495692546944874, score: 0.65625\n",
      "[Dev] loss: 2.329149063384439, score: 0.826\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: 1.764158731523012, score: 0.875\n",
      "[Dev] loss: 2.3167005122890916, score: 0.826\n",
      "epoch: 3, iteration: 1500\n",
      "[Train] loss: 1.8983486849064277, score: 0.78125\n",
      "[Dev] loss: 2.3096348910444773, score: 0.827\n",
      "best accuracy performence has been updated: 0.82100 --> 0.82700\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: 2.72088142931856, score: 0.8125\n",
      "[Dev] loss: 2.314190665604003, score: 0.827\n",
      "epoch: 4, iteration: 100\n",
      "[Train] loss: 2.9563704893814915, score: 0.8125\n",
      "[Dev] loss: 2.3186969560095374, score: 0.828\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 2.7051442750932293, score: 0.84375\n",
      "[Dev] loss: 2.3082868517807453, score: 0.827\n",
      "epoch: 4, iteration: 300\n",
      "[Train] loss: 1.0718346521819837, score: 0.9375\n",
      "[Dev] loss: 2.312472705467654, score: 0.828\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 1.451640267884484, score: 0.90625\n",
      "[Dev] loss: 2.3083397202307934, score: 0.827\n",
      "epoch: 4, iteration: 500\n",
      "[Train] loss: 3.227637058566256, score: 0.8125\n",
      "[Dev] loss: 2.3018287938565147, score: 0.827\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: 1.4162453887692483, score: 0.875\n",
      "[Dev] loss: 2.2856753624701907, score: 0.826\n",
      "epoch: 4, iteration: 700\n",
      "[Train] loss: 1.4875461051454053, score: 0.875\n",
      "[Dev] loss: 2.286678569243517, score: 0.827\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 3.243346874989892, score: 0.78125\n",
      "[Dev] loss: 2.270974220735273, score: 0.827\n",
      "epoch: 4, iteration: 900\n",
      "[Train] loss: 2.7464705594717813, score: 0.78125\n",
      "[Dev] loss: 2.272459367355411, score: 0.83\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: 2.30338841159871, score: 0.875\n",
      "[Dev] loss: 2.2634622650128424, score: 0.827\n",
      "epoch: 4, iteration: 1100\n",
      "[Train] loss: 3.845949759778505, score: 0.6875\n",
      "[Dev] loss: 2.256090451462951, score: 0.833\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: 2.5439832800267568, score: 0.78125\n",
      "[Dev] loss: 2.2536232907374707, score: 0.831\n",
      "epoch: 4, iteration: 1300\n",
      "[Train] loss: 1.7970428752824086, score: 0.78125\n",
      "[Dev] loss: 2.2544527193314665, score: 0.831\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: 0.9714335013848509, score: 0.875\n",
      "[Dev] loss: 2.248040398277229, score: 0.832\n",
      "epoch: 4, iteration: 1500\n",
      "[Train] loss: 2.032240598837739, score: 0.84375\n",
      "[Dev] loss: 2.2443019857199435, score: 0.835\n",
      "best accuracy performence has been updated: 0.82700 --> 0.83000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:02:20.135032Z",
     "start_time": "2025-04-04T03:35:56.681094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\"\"\"       conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2),  # 输出 [batch, 6, 28, 28]\n",
    "\n",
    "            conv2D(in_channels=6, out_channels=16, kernel_size=3, padding=0)\n",
    "\n",
    "            Flatten(),\n",
    "            \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=20, log_iters=100, save_dir=r'./best_models')"
   ],
   "id": "28150363ea6a7780",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 17.269388196830345, score: 0.0625\n",
      "[Dev] loss: 16.454170808219526, score: 0.103125\n",
      "epoch: 0, iteration: 100\n",
      "[Train] loss: 3.453564479136238, score: 0.8125\n",
      "[Dev] loss: 4.21972055602634, score: 0.75625\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 1.7457439243595938, score: 0.90625\n",
      "[Dev] loss: 3.544004452753788, score: 0.796875\n",
      "epoch: 0, iteration: 300\n",
      "[Train] loss: 2.879118023998947, score: 0.84375\n",
      "[Dev] loss: 2.5964218710617786, score: 0.853125\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 2.302585084244046, score: 0.875\n",
      "[Dev] loss: 2.704101647283122, score: 0.84375\n",
      "epoch: 0, iteration: 500\n",
      "[Train] loss: 3.4446682082246207, score: 0.8125\n",
      "[Dev] loss: 2.6719512462270645, score: 0.84375\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 2.33486720547986, score: 0.84375\n",
      "[Dev] loss: 2.3553238923997406, score: 0.859375\n",
      "epoch: 0, iteration: 700\n",
      "[Train] loss: 1.7736552814218145, score: 0.875\n",
      "[Dev] loss: 1.9646748459671186, score: 0.884375\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 1.3504789400281014, score: 0.875\n",
      "[Dev] loss: 1.971191557037764, score: 0.88125\n",
      "epoch: 0, iteration: 900\n",
      "[Train] loss: 4.029619477041814, score: 0.78125\n",
      "[Dev] loss: 2.093867579466646, score: 0.875\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 3.0045112693919878, score: 0.8125\n",
      "[Dev] loss: 1.807779183721447, score: 0.890625\n",
      "epoch: 0, iteration: 1100\n",
      "[Train] loss: 2.535888414850582, score: 0.84375\n",
      "[Dev] loss: 1.8615093757458232, score: 0.878125\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 2.481454662356515, score: 0.84375\n",
      "[Dev] loss: 1.7423405279624116, score: 0.8875\n",
      "epoch: 0, iteration: 1300\n",
      "[Train] loss: 3.050904552502911, score: 0.8125\n",
      "[Dev] loss: 1.9434394351435664, score: 0.865625\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 1.696147160625074, score: 0.875\n",
      "[Dev] loss: 1.7223466545953712, score: 0.890625\n",
      "epoch: 0, iteration: 1500\n",
      "[Train] loss: 4.031082272626116, score: 0.78125\n",
      "[Dev] loss: 1.8219414127162736, score: 0.884375\n",
      "best accuracy performence has been updated: 0.00000 --> 0.78125\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 4.17604301525937, score: 0.65625\n",
      "[Dev] loss: 2.3609009743797493, score: 0.834375\n",
      "epoch: 1, iteration: 100\n",
      "[Train] loss: 0.5762158317145385, score: 0.96875\n",
      "[Dev] loss: 1.610520373419977, score: 0.896875\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 3.654736132940679, score: 0.78125\n",
      "[Dev] loss: 1.5172766073029207, score: 0.9\n",
      "epoch: 1, iteration: 300\n",
      "[Train] loss: 1.151292704265685, score: 0.9375\n",
      "[Dev] loss: 1.858388569502252, score: 0.88125\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 2.371910073889562, score: 0.84375\n",
      "[Dev] loss: 1.7581377609082753, score: 0.8875\n",
      "epoch: 1, iteration: 500\n",
      "[Train] loss: 2.469469574174222, score: 0.84375\n",
      "[Dev] loss: 1.5627926909433427, score: 0.896875\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 1.292419365170899, score: 0.875\n",
      "[Dev] loss: 1.6474327736157846, score: 0.8875\n",
      "epoch: 1, iteration: 700\n",
      "[Train] loss: 1.7269387739183935, score: 0.90625\n",
      "[Dev] loss: 1.5636903167759277, score: 0.89375\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 0.5843890833989358, score: 0.96875\n",
      "[Dev] loss: 1.6894714986881731, score: 0.878125\n",
      "epoch: 1, iteration: 900\n",
      "[Train] loss: 1.3665730304613855, score: 0.90625\n",
      "[Dev] loss: 1.3973164541658214, score: 0.909375\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 2.773844837801756, score: 0.8125\n",
      "[Dev] loss: 1.4248487073767349, score: 0.9125\n",
      "epoch: 1, iteration: 1100\n",
      "[Train] loss: 3.6865015562923427, score: 0.78125\n",
      "[Dev] loss: 1.5321088954247868, score: 0.89375\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 0.831493328988393, score: 0.9375\n",
      "[Dev] loss: 1.4049150059200342, score: 0.90625\n",
      "epoch: 1, iteration: 1300\n",
      "[Train] loss: 2.661257001594049, score: 0.8125\n",
      "[Dev] loss: 1.5690760366994794, score: 0.884375\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: 1.7097354019994977, score: 0.875\n",
      "[Dev] loss: 1.6240211026263822, score: 0.878125\n",
      "epoch: 1, iteration: 1500\n",
      "[Train] loss: 2.9322995403259364, score: 0.78125\n",
      "[Dev] loss: 1.381952307035984, score: 0.89375\n",
      "best accuracy performence has been updated: 0.78125 --> 0.90312\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 2.45939321299863, score: 0.84375\n",
      "[Dev] loss: 1.4201093967591256, score: 0.903125\n",
      "epoch: 2, iteration: 100\n",
      "[Train] loss: 2.5606565596437654, score: 0.84375\n",
      "[Dev] loss: 1.449500604441301, score: 0.903125\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: 2.292213789060988, score: 0.875\n",
      "[Dev] loss: 1.5544332192549308, score: 0.9\n",
      "epoch: 2, iteration: 300\n",
      "[Train] loss: 2.0653684592434525, score: 0.84375\n",
      "[Dev] loss: 1.603210454075149, score: 0.8875\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 1.225144078251229, score: 0.90625\n",
      "[Dev] loss: 1.3654198415629906, score: 0.909375\n",
      "epoch: 2, iteration: 500\n",
      "[Train] loss: 0.7214327942594884, score: 0.90625\n",
      "[Dev] loss: 1.436685468525016, score: 0.903125\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 1.2913103469911447, score: 0.90625\n",
      "[Dev] loss: 1.3232741723338042, score: 0.915625\n",
      "epoch: 2, iteration: 700\n",
      "[Train] loss: 2.7747794236438317, score: 0.78125\n",
      "[Dev] loss: 1.3025508559448906, score: 0.909375\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 1.9348843647692011, score: 0.875\n",
      "[Dev] loss: 1.3654708347885378, score: 0.9125\n",
      "epoch: 2, iteration: 900\n",
      "[Train] loss: 2.5223164971515146, score: 0.84375\n",
      "[Dev] loss: 1.3212783913289807, score: 0.9125\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 1.151292575459264, score: 0.9375\n",
      "[Dev] loss: 1.229176566417079, score: 0.9125\n",
      "epoch: 2, iteration: 1100\n",
      "[Train] loss: 1.617138243314286, score: 0.84375\n",
      "[Dev] loss: 1.3961029379090704, score: 0.90625\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 1.8914310415038513, score: 0.84375\n",
      "[Dev] loss: 1.325764423438148, score: 0.915625\n",
      "epoch: 2, iteration: 1300\n",
      "[Train] loss: 1.9149971783513244, score: 0.84375\n",
      "[Dev] loss: 1.3937206087753446, score: 0.909375\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: 1.320035647198742, score: 0.875\n",
      "[Dev] loss: 1.4174197150632708, score: 0.90625\n",
      "epoch: 2, iteration: 1500\n",
      "[Train] loss: 1.72693885911377, score: 0.90625\n",
      "[Dev] loss: 1.4411331700260326, score: 0.90625\n",
      "best accuracy performence has been updated: 0.90312 --> 0.91563\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 1.1520658443723766, score: 0.9375\n",
      "[Dev] loss: 1.3447308893597099, score: 0.9125\n",
      "epoch: 3, iteration: 100\n",
      "[Train] loss: 1.3785576856913389, score: 0.90625\n",
      "[Dev] loss: 1.2792954237009309, score: 0.91875\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: 1.7274099262889147, score: 0.90625\n",
      "[Dev] loss: 1.3351057654348153, score: 0.90625\n",
      "epoch: 3, iteration: 300\n",
      "[Train] loss: 0.6967166029947525, score: 0.9375\n",
      "[Dev] loss: 1.3011853253042707, score: 0.915625\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 1.5832577563250692, score: 0.875\n",
      "[Dev] loss: 1.2816417668877655, score: 0.915625\n",
      "epoch: 3, iteration: 500\n",
      "[Train] loss: 2.407559264590069, score: 0.84375\n",
      "[Dev] loss: 1.3397845494393061, score: 0.9125\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 1.7020209396227712, score: 0.84375\n",
      "[Dev] loss: 1.3306144911701747, score: 0.909375\n",
      "epoch: 3, iteration: 700\n",
      "[Train] loss: 2.122039736227772, score: 0.84375\n",
      "[Dev] loss: 1.3625426178526505, score: 0.909375\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 1.1513180433914263, score: 0.9375\n",
      "[Dev] loss: 1.2769889821648268, score: 0.91875\n",
      "epoch: 3, iteration: 900\n",
      "[Train] loss: 1.822050587658774, score: 0.875\n",
      "[Dev] loss: 1.3960918192159877, score: 0.9\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 0.5916507277884514, score: 0.9375\n",
      "[Dev] loss: 1.3193233425331088, score: 0.9125\n",
      "epoch: 3, iteration: 1100\n",
      "[Train] loss: 1.3013074603552481, score: 0.90625\n",
      "[Dev] loss: 1.3395696321623016, score: 0.90625\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: 2.34900228548182, score: 0.8125\n",
      "[Dev] loss: 1.2523591508147451, score: 0.909375\n",
      "epoch: 3, iteration: 1300\n",
      "[Train] loss: 3.2247977767723066, score: 0.78125\n",
      "[Dev] loss: 1.292687799028831, score: 0.90625\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: 0.5306584424525237, score: 0.96875\n",
      "[Dev] loss: 1.3602857955098757, score: 0.90625\n",
      "epoch: 3, iteration: 1500\n",
      "[Train] loss: 1.2745496874417044, score: 0.90625\n",
      "[Dev] loss: 1.269712132168692, score: 0.9125\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: 0.6759326276147881, score: 0.9375\n",
      "[Dev] loss: 1.3193665379658164, score: 0.903125\n",
      "epoch: 4, iteration: 100\n",
      "[Train] loss: 1.618381281708718, score: 0.875\n",
      "[Dev] loss: 1.389995265850983, score: 0.9\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 1.2595562887028824, score: 0.90625\n",
      "[Dev] loss: 1.297492303744172, score: 0.909375\n",
      "epoch: 4, iteration: 300\n",
      "[Train] loss: 1.2309799986478305, score: 0.84375\n",
      "[Dev] loss: 1.2756990133549162, score: 0.909375\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 0.7952755060644185, score: 0.9375\n",
      "[Dev] loss: 1.2718684176008153, score: 0.9125\n",
      "epoch: 4, iteration: 500\n",
      "[Train] loss: 1.2563341125500251, score: 0.90625\n",
      "[Dev] loss: 1.2084241494765842, score: 0.90625\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: 2.3082106894021446, score: 0.84375\n",
      "[Dev] loss: 1.2828594685713839, score: 0.90625\n",
      "epoch: 4, iteration: 700\n",
      "[Train] loss: 2.008507795189692, score: 0.875\n",
      "[Dev] loss: 1.211452094275955, score: 0.903125\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.8372588310891063, score: 0.84375\n",
      "[Dev] loss: 1.3535567731591656, score: 0.896875\n",
      "epoch: 4, iteration: 900\n",
      "[Train] loss: 1.796195695045468, score: 0.84375\n",
      "[Dev] loss: 1.3113198806913868, score: 0.89375\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: 2.7006933181377466, score: 0.8125\n",
      "[Dev] loss: 1.309570447069238, score: 0.903125\n",
      "epoch: 4, iteration: 1100\n",
      "[Train] loss: 1.875712875280954, score: 0.84375\n",
      "[Dev] loss: 1.2754245434015103, score: 0.90625\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: 0.9527489028854951, score: 0.90625\n",
      "[Dev] loss: 1.2880881015678174, score: 0.90625\n",
      "epoch: 4, iteration: 1300\n",
      "[Train] loss: 0.7962691957262233, score: 0.90625\n",
      "[Dev] loss: 1.2927516896222937, score: 0.9125\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: 4.165694615230481, score: 0.75\n",
      "[Dev] loss: 1.3257519685883548, score: 0.90625\n",
      "epoch: 4, iteration: 1500\n",
      "[Train] loss: 1.7919824042189463, score: 0.875\n",
      "[Dev] loss: 1.2777089973981564, score: 0.909375\n",
      "epoch: 5, iteration: 0\n",
      "[Train] loss: 0.5778791032261076, score: 0.96875\n",
      "[Dev] loss: 1.2338065288233468, score: 0.909375\n",
      "epoch: 5, iteration: 100\n",
      "[Train] loss: 0.7615150304652927, score: 0.90625\n",
      "[Dev] loss: 1.2240808314137082, score: 0.90625\n",
      "epoch: 5, iteration: 200\n",
      "[Train] loss: 1.945838690936181, score: 0.84375\n",
      "[Dev] loss: 1.2411768064631974, score: 0.909375\n",
      "epoch: 5, iteration: 300\n",
      "[Train] loss: 0.5800736069551587, score: 0.9375\n",
      "[Dev] loss: 1.2083013870888943, score: 0.9\n",
      "epoch: 5, iteration: 400\n",
      "[Train] loss: 0.7529106266956702, score: 0.9375\n",
      "[Dev] loss: 1.2873650308672722, score: 0.903125\n",
      "epoch: 5, iteration: 500\n",
      "[Train] loss: 3.0357989388444118, score: 0.8125\n",
      "[Dev] loss: 1.2243867182901038, score: 0.90625\n",
      "epoch: 5, iteration: 600\n",
      "[Train] loss: 1.3405517507584914, score: 0.84375\n",
      "[Dev] loss: 1.3141584406984805, score: 0.903125\n",
      "epoch: 5, iteration: 700\n",
      "[Train] loss: 1.8910748212282815, score: 0.875\n",
      "[Dev] loss: 1.3337624319062678, score: 0.9\n",
      "epoch: 5, iteration: 800\n",
      "[Train] loss: 1.1587735947706312, score: 0.9375\n",
      "[Dev] loss: 1.2884515611540972, score: 0.903125\n",
      "epoch: 5, iteration: 900\n",
      "[Train] loss: 1.2239962764324424, score: 0.90625\n",
      "[Dev] loss: 1.2803623062632985, score: 0.903125\n",
      "epoch: 5, iteration: 1000\n",
      "[Train] loss: 0.00016427224952054233, score: 1.0\n",
      "[Dev] loss: 1.2595324070190876, score: 0.909375\n",
      "epoch: 5, iteration: 1100\n",
      "[Train] loss: 1.371833113647753, score: 0.875\n",
      "[Dev] loss: 1.2690690688973831, score: 0.903125\n",
      "epoch: 5, iteration: 1200\n",
      "[Train] loss: 0.5770578398472865, score: 0.96875\n",
      "[Dev] loss: 1.2558806546744392, score: 0.903125\n",
      "epoch: 5, iteration: 1300\n",
      "[Train] loss: 0.05595129739614555, score: 0.96875\n",
      "[Dev] loss: 1.285836372693527, score: 0.9\n",
      "epoch: 5, iteration: 1400\n",
      "[Train] loss: 0.003808576206965865, score: 1.0\n",
      "[Dev] loss: 1.2612829743815173, score: 0.903125\n",
      "epoch: 5, iteration: 1500\n",
      "[Train] loss: 0.9144292764957336, score: 0.90625\n",
      "[Dev] loss: 1.2512801653459704, score: 0.903125\n",
      "epoch: 6, iteration: 0\n",
      "[Train] loss: 0.628654889371122, score: 0.9375\n",
      "[Dev] loss: 1.241989799689789, score: 0.903125\n",
      "epoch: 6, iteration: 100\n",
      "[Train] loss: 2.948712366500038, score: 0.8125\n",
      "[Dev] loss: 1.248193518777367, score: 0.903125\n",
      "epoch: 6, iteration: 200\n",
      "[Train] loss: 0.5756463688555616, score: 0.96875\n",
      "[Dev] loss: 1.197259318336129, score: 0.903125\n",
      "epoch: 6, iteration: 300\n",
      "[Train] loss: 1.7177426738740649, score: 0.90625\n",
      "[Dev] loss: 1.24263366068388, score: 0.9\n",
      "epoch: 6, iteration: 400\n",
      "[Train] loss: 2.4704473098522306, score: 0.84375\n",
      "[Dev] loss: 1.238915125679944, score: 0.903125\n",
      "epoch: 6, iteration: 500\n",
      "[Train] loss: 2.857785023844504, score: 0.84375\n",
      "[Dev] loss: 1.2270456638505514, score: 0.90625\n",
      "epoch: 6, iteration: 600\n",
      "[Train] loss: 0.6369354832583206, score: 0.9375\n",
      "[Dev] loss: 1.2366935851841194, score: 0.903125\n",
      "epoch: 6, iteration: 700\n",
      "[Train] loss: 2.1307371216176665, score: 0.78125\n",
      "[Dev] loss: 1.2538150030346873, score: 0.9\n",
      "epoch: 6, iteration: 800\n",
      "[Train] loss: 1.734381462699954, score: 0.90625\n",
      "[Dev] loss: 1.3569406191746205, score: 0.890625\n",
      "epoch: 6, iteration: 900\n",
      "[Train] loss: 0.5861205233791923, score: 0.96875\n",
      "[Dev] loss: 1.2781070577666862, score: 0.90625\n",
      "epoch: 6, iteration: 1000\n",
      "[Train] loss: 0.9370490307886719, score: 0.84375\n",
      "[Dev] loss: 1.3129570690152526, score: 0.896875\n",
      "epoch: 6, iteration: 1100\n",
      "[Train] loss: 0.8257010427705745, score: 0.90625\n",
      "[Dev] loss: 1.3061561963467554, score: 0.89375\n",
      "epoch: 6, iteration: 1200\n",
      "[Train] loss: 1.1512865500066978, score: 0.9375\n",
      "[Dev] loss: 1.2861026016048918, score: 0.89375\n",
      "epoch: 6, iteration: 1300\n",
      "[Train] loss: 1.9831615462539423, score: 0.875\n",
      "[Dev] loss: 1.236083981895541, score: 0.9\n",
      "epoch: 6, iteration: 1400\n",
      "[Train] loss: 0.5044196116964814, score: 0.9375\n",
      "[Dev] loss: 1.3355229058211164, score: 0.89375\n",
      "epoch: 6, iteration: 1500\n",
      "[Train] loss: 1.3935343083888732, score: 0.875\n",
      "[Dev] loss: 1.1620052972772552, score: 0.903125\n",
      "epoch: 7, iteration: 0\n",
      "[Train] loss: 0.8332280026063753, score: 0.90625\n",
      "[Dev] loss: 1.1747331496179712, score: 0.903125\n",
      "epoch: 7, iteration: 100\n",
      "[Train] loss: 1.7094082135234774, score: 0.875\n",
      "[Dev] loss: 1.2800380263284663, score: 0.9\n",
      "epoch: 7, iteration: 200\n",
      "[Train] loss: 1.2281603869309654, score: 0.875\n",
      "[Dev] loss: 1.2991887643000197, score: 0.896875\n",
      "epoch: 7, iteration: 300\n",
      "[Train] loss: 2.587673508901889, score: 0.84375\n",
      "[Dev] loss: 1.2503594117949017, score: 0.896875\n",
      "epoch: 7, iteration: 400\n",
      "[Train] loss: 2.010861665591226, score: 0.875\n",
      "[Dev] loss: 1.2232233352271749, score: 0.9\n",
      "epoch: 7, iteration: 500\n",
      "[Train] loss: 1.0416263336792673, score: 0.9375\n",
      "[Dev] loss: 1.310943196116273, score: 0.903125\n",
      "epoch: 7, iteration: 600\n",
      "[Train] loss: 0.6145847932513007, score: 0.9375\n",
      "[Dev] loss: 1.2539969879916177, score: 0.903125\n",
      "epoch: 7, iteration: 700\n",
      "[Train] loss: 2.3384081997848956, score: 0.84375\n",
      "[Dev] loss: 1.2891290108064477, score: 0.9\n",
      "epoch: 7, iteration: 800\n",
      "[Train] loss: 0.7374804048968002, score: 0.90625\n",
      "[Dev] loss: 1.1562065927560876, score: 0.909375\n",
      "epoch: 7, iteration: 900\n",
      "[Train] loss: 1.7472237125406402, score: 0.875\n",
      "[Dev] loss: 1.2010953636868738, score: 0.903125\n",
      "epoch: 7, iteration: 1000\n",
      "[Train] loss: 0.16721009123817365, score: 0.9375\n",
      "[Dev] loss: 1.2843125820399819, score: 0.89375\n",
      "epoch: 7, iteration: 1100\n",
      "[Train] loss: 1.1438574185091932, score: 0.9375\n",
      "[Dev] loss: 1.2664539674154416, score: 0.9\n",
      "epoch: 7, iteration: 1200\n",
      "[Train] loss: 1.4162466261117808, score: 0.875\n",
      "[Dev] loss: 1.233201728703285, score: 0.903125\n",
      "epoch: 7, iteration: 1300\n",
      "[Train] loss: 0.6657329465060713, score: 0.90625\n",
      "[Dev] loss: 1.2079344595387076, score: 0.903125\n",
      "epoch: 7, iteration: 1400\n",
      "[Train] loss: 1.8802901841403685, score: 0.84375\n",
      "[Dev] loss: 1.1928976309006878, score: 0.903125\n",
      "epoch: 7, iteration: 1500\n",
      "[Train] loss: 1.4935875634607414, score: 0.84375\n",
      "[Dev] loss: 1.2086671251566916, score: 0.896875\n",
      "epoch: 8, iteration: 0\n",
      "[Train] loss: 0.8346001609284921, score: 0.9375\n",
      "[Dev] loss: 1.166523919429573, score: 0.903125\n",
      "epoch: 8, iteration: 100\n",
      "[Train] loss: 1.8592984560898649, score: 0.875\n",
      "[Dev] loss: 1.1888934094980546, score: 0.903125\n",
      "epoch: 8, iteration: 200\n",
      "[Train] loss: 1.1870883065888185, score: 0.90625\n",
      "[Dev] loss: 1.2074906004424035, score: 0.903125\n",
      "epoch: 8, iteration: 300\n",
      "[Train] loss: 1.691299545358627, score: 0.875\n",
      "[Dev] loss: 1.229674386423253, score: 0.9\n",
      "epoch: 8, iteration: 400\n",
      "[Train] loss: 0.9628904519020313, score: 0.90625\n",
      "[Dev] loss: 1.1868547063673551, score: 0.9\n",
      "epoch: 8, iteration: 500\n",
      "[Train] loss: 0.35196649446030276, score: 0.96875\n",
      "[Dev] loss: 1.192522218303683, score: 0.9\n",
      "epoch: 8, iteration: 600\n",
      "[Train] loss: 1.417140164477476, score: 0.90625\n",
      "[Dev] loss: 1.1711399364275454, score: 0.9\n",
      "epoch: 8, iteration: 700\n",
      "[Train] loss: 1.5761233576817162, score: 0.875\n",
      "[Dev] loss: 1.2011400444398768, score: 0.903125\n",
      "epoch: 8, iteration: 800\n",
      "[Train] loss: 1.593562603166508, score: 0.875\n",
      "[Dev] loss: 1.2506168723619575, score: 0.9\n",
      "epoch: 8, iteration: 900\n",
      "[Train] loss: 1.721914284686134, score: 0.90625\n",
      "[Dev] loss: 1.2443338144783942, score: 0.9\n",
      "epoch: 8, iteration: 1000\n",
      "[Train] loss: 0.42433303760151, score: 0.9375\n",
      "[Dev] loss: 1.248841227518764, score: 0.896875\n",
      "epoch: 8, iteration: 1100\n",
      "[Train] loss: 0.8651373686798849, score: 0.90625\n",
      "[Dev] loss: 1.259312998458904, score: 0.896875\n",
      "epoch: 8, iteration: 1200\n",
      "[Train] loss: 2.8792661117902236, score: 0.8125\n",
      "[Dev] loss: 1.1831427283061777, score: 0.9\n",
      "epoch: 8, iteration: 1300\n",
      "[Train] loss: 0.9558571035934885, score: 0.875\n",
      "[Dev] loss: 1.2491125852897966, score: 0.9\n",
      "epoch: 8, iteration: 1400\n",
      "[Train] loss: 0.7112995357528944, score: 0.9375\n",
      "[Dev] loss: 1.2281719473713837, score: 0.9\n",
      "epoch: 8, iteration: 1500\n",
      "[Train] loss: 1.774388473593253, score: 0.875\n",
      "[Dev] loss: 1.1859499441132493, score: 0.903125\n",
      "epoch: 9, iteration: 0\n",
      "[Train] loss: 0.23310293098280058, score: 0.9375\n",
      "[Dev] loss: 1.1064703218904928, score: 0.903125\n",
      "epoch: 9, iteration: 100\n",
      "[Train] loss: 1.4869128123030335, score: 0.84375\n",
      "[Dev] loss: 1.1853092615813428, score: 0.903125\n",
      "epoch: 9, iteration: 200\n",
      "[Train] loss: 0.9362121950968227, score: 0.90625\n",
      "[Dev] loss: 1.204328358150955, score: 0.9\n",
      "epoch: 9, iteration: 300\n",
      "[Train] loss: 0.4168485877358089, score: 0.96875\n",
      "[Dev] loss: 1.1458583095128678, score: 0.9\n",
      "epoch: 9, iteration: 400\n",
      "[Train] loss: 2.704305890553284, score: 0.8125\n",
      "[Dev] loss: 1.1775886577341246, score: 0.90625\n",
      "epoch: 9, iteration: 500\n",
      "[Train] loss: 0.3964549907257114, score: 0.96875\n",
      "[Dev] loss: 1.2609449449226209, score: 0.896875\n",
      "epoch: 9, iteration: 600\n",
      "[Train] loss: 1.6628175466760695, score: 0.84375\n",
      "[Dev] loss: 1.2290290267402497, score: 0.896875\n",
      "epoch: 9, iteration: 700\n",
      "[Train] loss: 1.7996196294754048, score: 0.8125\n",
      "[Dev] loss: 1.2322181203438587, score: 0.896875\n",
      "epoch: 9, iteration: 800\n",
      "[Train] loss: 0.8087284323178416, score: 0.875\n",
      "[Dev] loss: 1.139503052630586, score: 0.896875\n",
      "epoch: 9, iteration: 900\n",
      "[Train] loss: 0.9117225458677851, score: 0.875\n",
      "[Dev] loss: 1.2200792635412085, score: 0.903125\n",
      "epoch: 9, iteration: 1000\n",
      "[Train] loss: 1.7270892819628103, score: 0.90625\n",
      "[Dev] loss: 1.1715059303940891, score: 0.9\n",
      "epoch: 9, iteration: 1100\n",
      "[Train] loss: 1.0167199003310936, score: 0.9375\n",
      "[Dev] loss: 1.1778720326870027, score: 0.896875\n",
      "epoch: 9, iteration: 1200\n",
      "[Train] loss: 1.3636379432424386, score: 0.875\n",
      "[Dev] loss: 1.2222896136781962, score: 0.896875\n",
      "epoch: 9, iteration: 1300\n",
      "[Train] loss: 1.6749993720169156, score: 0.875\n",
      "[Dev] loss: 1.2074959308496027, score: 0.9\n",
      "epoch: 9, iteration: 1400\n",
      "[Train] loss: 3.1345308544931694, score: 0.8125\n",
      "[Dev] loss: 1.1786904302566914, score: 0.9\n",
      "epoch: 9, iteration: 1500\n",
      "[Train] loss: 1.7183637516837789, score: 0.84375\n",
      "[Dev] loss: 1.2144616120970504, score: 0.896875\n",
      "epoch: 10, iteration: 0\n",
      "[Train] loss: 0.6119388931742239, score: 0.9375\n",
      "[Dev] loss: 1.1391197811039324, score: 0.9\n",
      "epoch: 10, iteration: 100\n",
      "[Train] loss: 1.3201814895097355, score: 0.875\n",
      "[Dev] loss: 1.1815466601263305, score: 0.9\n",
      "epoch: 10, iteration: 200\n",
      "[Train] loss: 3.0980404283284493, score: 0.75\n",
      "[Dev] loss: 1.1786767513337615, score: 0.896875\n",
      "epoch: 10, iteration: 300\n",
      "[Train] loss: 1.4425289384375248, score: 0.90625\n",
      "[Dev] loss: 1.1978280448665415, score: 0.896875\n",
      "epoch: 10, iteration: 400\n",
      "[Train] loss: 1.7488379051076368, score: 0.875\n",
      "[Dev] loss: 1.2301043931320368, score: 0.9\n",
      "epoch: 10, iteration: 500\n",
      "[Train] loss: 0.2300384952132255, score: 0.9375\n",
      "[Dev] loss: 1.135554050694499, score: 0.903125\n",
      "epoch: 10, iteration: 600\n",
      "[Train] loss: 0.912712243070829, score: 0.90625\n",
      "[Dev] loss: 1.167414508855584, score: 0.9\n",
      "epoch: 10, iteration: 700\n",
      "[Train] loss: 1.5110200898526642, score: 0.875\n",
      "[Dev] loss: 1.1460618729581449, score: 0.9\n",
      "epoch: 10, iteration: 800\n",
      "[Train] loss: 0.23484803850839214, score: 0.90625\n",
      "[Dev] loss: 1.2588808769369897, score: 0.9\n",
      "epoch: 10, iteration: 900\n",
      "[Train] loss: 0.0537250337225391, score: 0.96875\n",
      "[Dev] loss: 1.2300387690105232, score: 0.9\n",
      "epoch: 10, iteration: 1000\n",
      "[Train] loss: 1.3281882301692565, score: 0.875\n",
      "[Dev] loss: 1.2503514086450447, score: 0.896875\n",
      "epoch: 10, iteration: 1100\n",
      "[Train] loss: 0.6003040801382449, score: 0.875\n",
      "[Dev] loss: 1.1394889027553075, score: 0.9\n",
      "epoch: 10, iteration: 1200\n",
      "[Train] loss: 0.005894942388870607, score: 1.0\n",
      "[Dev] loss: 1.191311899286828, score: 0.9\n",
      "epoch: 10, iteration: 1300\n",
      "[Train] loss: 1.6664071336993607, score: 0.875\n",
      "[Dev] loss: 1.180430697525456, score: 0.9\n",
      "epoch: 10, iteration: 1400\n",
      "[Train] loss: 1.462687009438863, score: 0.875\n",
      "[Dev] loss: 1.2145550295564924, score: 0.9\n",
      "epoch: 10, iteration: 1500\n",
      "[Train] loss: 1.0294500859513112, score: 0.9375\n",
      "[Dev] loss: 1.2363241326303092, score: 0.9\n",
      "epoch: 11, iteration: 0\n",
      "[Train] loss: 2.255367542680947, score: 0.8125\n",
      "[Dev] loss: 1.21361084402529, score: 0.9\n",
      "epoch: 11, iteration: 100\n",
      "[Train] loss: 1.3739504178743356, score: 0.875\n",
      "[Dev] loss: 1.1714676478516908, score: 0.9\n",
      "epoch: 11, iteration: 200\n",
      "[Train] loss: 0.7158365261454681, score: 0.90625\n",
      "[Dev] loss: 1.2028309414978933, score: 0.9\n",
      "epoch: 11, iteration: 300\n",
      "[Train] loss: 2.176683739468656, score: 0.84375\n",
      "[Dev] loss: 1.2403810600631615, score: 0.9\n",
      "epoch: 11, iteration: 400\n",
      "[Train] loss: 0.8491344744386494, score: 0.9375\n",
      "[Dev] loss: 1.157106162746314, score: 0.9\n",
      "epoch: 11, iteration: 500\n",
      "[Train] loss: 3.0583170543314515, score: 0.75\n",
      "[Dev] loss: 1.2788328796288664, score: 0.896875\n",
      "epoch: 11, iteration: 600\n",
      "[Train] loss: 2.337263914657176, score: 0.8125\n",
      "[Dev] loss: 1.15623214635488, score: 0.903125\n",
      "epoch: 11, iteration: 700\n",
      "[Train] loss: 0.5895336847338077, score: 0.90625\n",
      "[Dev] loss: 1.188139820543434, score: 0.9\n",
      "epoch: 11, iteration: 800\n",
      "[Train] loss: 1.7999834268597434, score: 0.8125\n",
      "[Dev] loss: 1.2145164467015381, score: 0.9\n",
      "epoch: 11, iteration: 900\n",
      "[Train] loss: 0.27669383628195654, score: 0.96875\n",
      "[Dev] loss: 1.194308309703854, score: 0.9\n",
      "epoch: 11, iteration: 1000\n",
      "[Train] loss: 2.598888106541034, score: 0.8125\n",
      "[Dev] loss: 1.1375448726654231, score: 0.903125\n",
      "epoch: 11, iteration: 1100\n",
      "[Train] loss: 1.0157436787524063, score: 0.9375\n",
      "[Dev] loss: 1.1423047802139876, score: 0.9\n",
      "epoch: 11, iteration: 1200\n",
      "[Train] loss: 0.11930615996164884, score: 0.96875\n",
      "[Dev] loss: 1.1133922269131964, score: 0.90625\n",
      "epoch: 11, iteration: 1300\n",
      "[Train] loss: 1.6493714832901556, score: 0.875\n",
      "[Dev] loss: 1.2305726374825634, score: 0.9\n",
      "epoch: 11, iteration: 1400\n",
      "[Train] loss: 1.4726726338663982, score: 0.90625\n",
      "[Dev] loss: 1.2694015022422926, score: 0.896875\n",
      "epoch: 11, iteration: 1500\n",
      "[Train] loss: 1.454088376564553, score: 0.84375\n",
      "[Dev] loss: 1.1582338754589951, score: 0.903125\n",
      "epoch: 12, iteration: 0\n",
      "[Train] loss: 0.05914879701393606, score: 0.96875\n",
      "[Dev] loss: 1.1888903640108424, score: 0.9\n",
      "epoch: 12, iteration: 100\n",
      "[Train] loss: 0.5929512762322813, score: 0.96875\n",
      "[Dev] loss: 1.1361493395773148, score: 0.90625\n",
      "epoch: 12, iteration: 200\n",
      "[Train] loss: 0.8117464136675482, score: 0.90625\n",
      "[Dev] loss: 1.2161716683686954, score: 0.896875\n",
      "epoch: 12, iteration: 300\n",
      "[Train] loss: 1.5163216115432758, score: 0.8125\n",
      "[Dev] loss: 1.1617374544235561, score: 0.90625\n",
      "epoch: 12, iteration: 400\n",
      "[Train] loss: 2.0659092456308707, score: 0.84375\n",
      "[Dev] loss: 1.1771197804965503, score: 0.903125\n",
      "epoch: 12, iteration: 500\n",
      "[Train] loss: 1.1150389961818994, score: 0.90625\n",
      "[Dev] loss: 1.1641014739882052, score: 0.9\n",
      "epoch: 12, iteration: 600\n",
      "[Train] loss: 0.6292795064824872, score: 0.9375\n",
      "[Dev] loss: 1.162830554699609, score: 0.9\n",
      "epoch: 12, iteration: 700\n",
      "[Train] loss: 2.5138635224367984, score: 0.84375\n",
      "[Dev] loss: 1.213376183847645, score: 0.896875\n",
      "epoch: 12, iteration: 800\n",
      "[Train] loss: 1.241572734081891, score: 0.90625\n",
      "[Dev] loss: 1.253348450856016, score: 0.8875\n",
      "epoch: 12, iteration: 900\n",
      "[Train] loss: 1.115748333579671, score: 0.875\n",
      "[Dev] loss: 1.1767250770164346, score: 0.896875\n",
      "epoch: 12, iteration: 1000\n",
      "[Train] loss: 1.63138053743317, score: 0.875\n",
      "[Dev] loss: 1.2176595607571206, score: 0.89375\n",
      "epoch: 12, iteration: 1100\n",
      "[Train] loss: 1.2243822507855249, score: 0.875\n",
      "[Dev] loss: 1.1941378667655158, score: 0.896875\n",
      "epoch: 12, iteration: 1200\n",
      "[Train] loss: 0.5760425901350531, score: 0.96875\n",
      "[Dev] loss: 1.2566272570129964, score: 0.9\n",
      "epoch: 12, iteration: 1300\n",
      "[Train] loss: 1.3707810213281708, score: 0.90625\n",
      "[Dev] loss: 1.218568096488605, score: 0.90625\n",
      "epoch: 12, iteration: 1400\n",
      "[Train] loss: 1.969373032839224, score: 0.8125\n",
      "[Dev] loss: 1.149093609335742, score: 0.90625\n",
      "epoch: 12, iteration: 1500\n",
      "[Train] loss: 1.233221551556058, score: 0.90625\n",
      "[Dev] loss: 1.1504914911792543, score: 0.90625\n",
      "epoch: 13, iteration: 0\n",
      "[Train] loss: 1.1638991449818117, score: 0.875\n",
      "[Dev] loss: 1.1729101677755729, score: 0.903125\n",
      "epoch: 13, iteration: 100\n",
      "[Train] loss: 1.7173412633769376, score: 0.875\n",
      "[Dev] loss: 1.1566730838815125, score: 0.9\n",
      "epoch: 13, iteration: 200\n",
      "[Train] loss: 1.2470362859379895, score: 0.90625\n",
      "[Dev] loss: 1.2177011487813878, score: 0.89375\n",
      "epoch: 13, iteration: 300\n",
      "[Train] loss: 1.2103908639273027, score: 0.90625\n",
      "[Dev] loss: 1.1344333616937017, score: 0.90625\n",
      "epoch: 13, iteration: 400\n",
      "[Train] loss: 0.4671397066835401, score: 0.96875\n",
      "[Dev] loss: 1.1389201066007497, score: 0.903125\n",
      "epoch: 13, iteration: 500\n",
      "[Train] loss: 0.6760965549817771, score: 0.9375\n",
      "[Dev] loss: 1.1243508210316964, score: 0.896875\n",
      "epoch: 13, iteration: 600\n",
      "[Train] loss: 1.6442401835363905, score: 0.84375\n",
      "[Dev] loss: 1.1136279392933914, score: 0.903125\n",
      "epoch: 13, iteration: 700\n",
      "[Train] loss: 0.9542044750881729, score: 0.875\n",
      "[Dev] loss: 1.1150212305125007, score: 0.90625\n",
      "epoch: 13, iteration: 800\n",
      "[Train] loss: 1.4201610772970055, score: 0.875\n",
      "[Dev] loss: 1.1672888402619186, score: 0.89375\n",
      "epoch: 13, iteration: 900\n",
      "[Train] loss: 1.6544179101216936, score: 0.84375\n",
      "[Dev] loss: 1.184014006008401, score: 0.89375\n",
      "epoch: 13, iteration: 1000\n",
      "[Train] loss: 1.8381338793795008, score: 0.875\n",
      "[Dev] loss: 1.1950534712099208, score: 0.896875\n",
      "epoch: 13, iteration: 1100\n",
      "[Train] loss: 0.10413181495542569, score: 0.96875\n",
      "[Dev] loss: 1.2364204929236546, score: 0.89375\n",
      "epoch: 13, iteration: 1200\n",
      "[Train] loss: 2.1110082254862923, score: 0.84375\n",
      "[Dev] loss: 1.1560235146048106, score: 0.909375\n",
      "epoch: 13, iteration: 1300\n",
      "[Train] loss: 1.8189555164729374, score: 0.84375\n",
      "[Dev] loss: 1.2590598577337158, score: 0.896875\n",
      "epoch: 13, iteration: 1400\n",
      "[Train] loss: 1.2855699176563493, score: 0.90625\n",
      "[Dev] loss: 1.171035379414556, score: 0.90625\n",
      "epoch: 13, iteration: 1500\n",
      "[Train] loss: 1.3014083067406026, score: 0.875\n",
      "[Dev] loss: 1.1139164618888848, score: 0.909375\n",
      "epoch: 14, iteration: 0\n",
      "[Train] loss: 1.2789202313403831, score: 0.875\n",
      "[Dev] loss: 1.1532610891530424, score: 0.909375\n",
      "epoch: 14, iteration: 100\n",
      "[Train] loss: 1.3197378809140847, score: 0.875\n",
      "[Dev] loss: 1.1269951127115103, score: 0.909375\n",
      "epoch: 14, iteration: 200\n",
      "[Train] loss: 1.4649842098401822, score: 0.84375\n",
      "[Dev] loss: 1.1899565301451591, score: 0.896875\n",
      "epoch: 14, iteration: 300\n",
      "[Train] loss: 1.0822299606696681, score: 0.90625\n",
      "[Dev] loss: 1.1397808787895183, score: 0.90625\n",
      "epoch: 14, iteration: 400\n",
      "[Train] loss: 0.7079272609580831, score: 0.9375\n",
      "[Dev] loss: 1.1835222696787067, score: 0.9\n",
      "epoch: 14, iteration: 500\n",
      "[Train] loss: 2.0501569450126267, score: 0.84375\n",
      "[Dev] loss: 1.1689475252536279, score: 0.90625\n",
      "epoch: 14, iteration: 600\n",
      "[Train] loss: 1.7268851008746793, score: 0.8125\n",
      "[Dev] loss: 1.125968776228883, score: 0.9125\n",
      "epoch: 14, iteration: 700\n",
      "[Train] loss: 0.609952729454653, score: 0.90625\n",
      "[Dev] loss: 1.1779443102900833, score: 0.903125\n",
      "epoch: 14, iteration: 800\n",
      "[Train] loss: 0.732673729070443, score: 0.9375\n",
      "[Dev] loss: 1.1117639364430427, score: 0.9125\n",
      "epoch: 14, iteration: 900\n",
      "[Train] loss: 1.2448670511478388, score: 0.84375\n",
      "[Dev] loss: 1.192979173272644, score: 0.903125\n",
      "epoch: 14, iteration: 1000\n",
      "[Train] loss: 1.2067180698913271, score: 0.78125\n",
      "[Dev] loss: 1.204244914228164, score: 0.903125\n",
      "epoch: 14, iteration: 1100\n",
      "[Train] loss: 1.8764269079859626, score: 0.875\n",
      "[Dev] loss: 1.1760290636956392, score: 0.903125\n",
      "epoch: 14, iteration: 1200\n",
      "[Train] loss: 1.3206357049241182, score: 0.90625\n",
      "[Dev] loss: 1.209109997339296, score: 0.896875\n",
      "epoch: 14, iteration: 1300\n",
      "[Train] loss: 0.5554560575207674, score: 0.875\n",
      "[Dev] loss: 1.1971781975844638, score: 0.903125\n",
      "epoch: 14, iteration: 1400\n",
      "[Train] loss: 0.31533756144178404, score: 0.90625\n",
      "[Dev] loss: 1.1215400230315933, score: 0.9125\n",
      "epoch: 14, iteration: 1500\n",
      "[Train] loss: 1.3973973135801725, score: 0.875\n",
      "[Dev] loss: 1.188459413324315, score: 0.896875\n",
      "epoch: 15, iteration: 0\n",
      "[Train] loss: 0.9501974299668262, score: 0.84375\n",
      "[Dev] loss: 1.1085462728969548, score: 0.9125\n",
      "epoch: 15, iteration: 100\n",
      "[Train] loss: 0.25339822949206736, score: 0.9375\n",
      "[Dev] loss: 1.2006448558659257, score: 0.89375\n",
      "epoch: 15, iteration: 200\n",
      "[Train] loss: 1.7384554190922077, score: 0.84375\n",
      "[Dev] loss: 1.158846357213402, score: 0.9\n",
      "epoch: 15, iteration: 300\n",
      "[Train] loss: 2.0656213422848673, score: 0.8125\n",
      "[Dev] loss: 1.1598290357246772, score: 0.90625\n",
      "epoch: 15, iteration: 400\n",
      "[Train] loss: 1.6300631607591458, score: 0.875\n",
      "[Dev] loss: 1.1932748816511047, score: 0.896875\n",
      "epoch: 15, iteration: 500\n",
      "[Train] loss: 1.174732755141722, score: 0.90625\n",
      "[Dev] loss: 1.20555865076512, score: 0.896875\n",
      "epoch: 15, iteration: 600\n",
      "[Train] loss: 0.8026784976165878, score: 0.90625\n",
      "[Dev] loss: 1.1180871612580614, score: 0.909375\n",
      "epoch: 15, iteration: 700\n",
      "[Train] loss: 0.5773120995393947, score: 0.96875\n",
      "[Dev] loss: 1.1297103199253713, score: 0.903125\n",
      "epoch: 15, iteration: 800\n",
      "[Train] loss: 1.5248867663537113, score: 0.875\n",
      "[Dev] loss: 1.1254995261986012, score: 0.903125\n",
      "epoch: 15, iteration: 900\n",
      "[Train] loss: 1.265698105037547, score: 0.8125\n",
      "[Dev] loss: 1.127877093773916, score: 0.90625\n",
      "epoch: 15, iteration: 1000\n",
      "[Train] loss: 0.7307617234044357, score: 0.9375\n",
      "[Dev] loss: 1.1372013966583254, score: 0.903125\n",
      "epoch: 15, iteration: 1100\n",
      "[Train] loss: 1.3397189057929997, score: 0.90625\n",
      "[Dev] loss: 1.1676259143174263, score: 0.896875\n",
      "epoch: 15, iteration: 1200\n",
      "[Train] loss: 2.1237109390024886, score: 0.84375\n",
      "[Dev] loss: 1.163197785721352, score: 0.89375\n",
      "epoch: 15, iteration: 1300\n",
      "[Train] loss: 1.175532744680424, score: 0.9375\n",
      "[Dev] loss: 1.134130367872936, score: 0.9\n",
      "epoch: 15, iteration: 1400\n",
      "[Train] loss: 2.1603534690755897, score: 0.78125\n",
      "[Dev] loss: 1.1592642207009332, score: 0.8875\n",
      "epoch: 15, iteration: 1500\n",
      "[Train] loss: 1.1513798940227042, score: 0.9375\n",
      "[Dev] loss: 1.1664670043087468, score: 0.9\n",
      "epoch: 16, iteration: 0\n",
      "[Train] loss: 0.22042622560684436, score: 0.96875\n",
      "[Dev] loss: 1.1808916299017134, score: 0.890625\n",
      "epoch: 16, iteration: 100\n",
      "[Train] loss: 1.5032221017342322, score: 0.84375\n",
      "[Dev] loss: 1.1516406975221474, score: 0.90625\n",
      "epoch: 16, iteration: 200\n",
      "[Train] loss: 0.7153415805281378, score: 0.9375\n",
      "[Dev] loss: 1.076948846845712, score: 0.903125\n",
      "epoch: 16, iteration: 300\n",
      "[Train] loss: 0.1551693149389476, score: 0.9375\n",
      "[Dev] loss: 1.1326956623521016, score: 0.909375\n",
      "epoch: 16, iteration: 400\n",
      "[Train] loss: 2.623193314477404, score: 0.8125\n",
      "[Dev] loss: 1.1095970981011598, score: 0.90625\n",
      "epoch: 16, iteration: 500\n",
      "[Train] loss: 1.507675364001287, score: 0.875\n",
      "[Dev] loss: 1.1311078441242999, score: 0.9\n",
      "epoch: 16, iteration: 600\n",
      "[Train] loss: 0.007757378098118548, score: 1.0\n",
      "[Dev] loss: 1.1494308009569252, score: 0.890625\n",
      "epoch: 16, iteration: 700\n",
      "[Train] loss: 0.5863773649666784, score: 0.96875\n",
      "[Dev] loss: 1.1554522051155025, score: 0.9\n",
      "epoch: 16, iteration: 800\n",
      "[Train] loss: 1.0188153801337392, score: 0.90625\n",
      "[Dev] loss: 1.1324167692109632, score: 0.896875\n",
      "epoch: 16, iteration: 900\n",
      "[Train] loss: 1.687920581588788, score: 0.875\n",
      "[Dev] loss: 1.1349329416383953, score: 0.9\n",
      "epoch: 16, iteration: 1000\n",
      "[Train] loss: 0.9487452805046341, score: 0.875\n",
      "[Dev] loss: 1.1347359166622948, score: 0.903125\n",
      "epoch: 16, iteration: 1100\n",
      "[Train] loss: 1.2659495401991736, score: 0.84375\n",
      "[Dev] loss: 1.1091004511586164, score: 0.9\n",
      "epoch: 16, iteration: 1200\n",
      "[Train] loss: 2.3801206193468536, score: 0.84375\n",
      "[Dev] loss: 1.1276958139968816, score: 0.9\n",
      "epoch: 16, iteration: 1300\n",
      "[Train] loss: 0.7151035518261054, score: 0.9375\n",
      "[Dev] loss: 1.09587270322731, score: 0.903125\n",
      "epoch: 16, iteration: 1400\n",
      "[Train] loss: 0.6737567685710648, score: 0.9375\n",
      "[Dev] loss: 1.109691707909955, score: 0.90625\n",
      "epoch: 16, iteration: 1500\n",
      "[Train] loss: 1.3027779174829928, score: 0.90625\n",
      "[Dev] loss: 1.1733842176898053, score: 0.90625\n",
      "epoch: 17, iteration: 0\n",
      "[Train] loss: 0.18800839231139713, score: 0.9375\n",
      "[Dev] loss: 1.1674653395308328, score: 0.90625\n",
      "epoch: 17, iteration: 100\n",
      "[Train] loss: 0.9439844203467327, score: 0.90625\n",
      "[Dev] loss: 1.118903781823834, score: 0.903125\n",
      "epoch: 17, iteration: 200\n",
      "[Train] loss: 1.0356790732104768, score: 0.90625\n",
      "[Dev] loss: 1.1027008170049974, score: 0.90625\n",
      "epoch: 17, iteration: 300\n",
      "[Train] loss: 1.3975192980362872, score: 0.90625\n",
      "[Dev] loss: 1.1303145447733611, score: 0.90625\n",
      "epoch: 17, iteration: 400\n",
      "[Train] loss: 2.0953602222141896, score: 0.84375\n",
      "[Dev] loss: 1.1071029030502264, score: 0.903125\n",
      "epoch: 17, iteration: 500\n",
      "[Train] loss: 1.2889531439411728, score: 0.90625\n",
      "[Dev] loss: 1.1330689913216598, score: 0.90625\n",
      "epoch: 17, iteration: 600\n",
      "[Train] loss: 0.3434044781225588, score: 0.9375\n",
      "[Dev] loss: 1.1116915858609029, score: 0.903125\n",
      "epoch: 17, iteration: 700\n",
      "[Train] loss: 1.2038998380115111, score: 0.90625\n",
      "[Dev] loss: 1.0985755525728882, score: 0.903125\n",
      "epoch: 17, iteration: 800\n",
      "[Train] loss: 2.247679051626239, score: 0.84375\n",
      "[Dev] loss: 1.1135397023871259, score: 0.896875\n",
      "epoch: 17, iteration: 900\n",
      "[Train] loss: 1.2464931319780388, score: 0.875\n",
      "[Dev] loss: 1.1392153924750763, score: 0.903125\n",
      "epoch: 17, iteration: 1000\n",
      "[Train] loss: 2.106701878004164, score: 0.84375\n",
      "[Dev] loss: 1.13165025463286, score: 0.896875\n",
      "epoch: 17, iteration: 1100\n",
      "[Train] loss: 0.8116051389177004, score: 0.90625\n",
      "[Dev] loss: 1.0795486553872466, score: 0.90625\n",
      "epoch: 17, iteration: 1200\n",
      "[Train] loss: 1.8656114658175968, score: 0.875\n",
      "[Dev] loss: 1.0736277157048186, score: 0.9\n",
      "epoch: 17, iteration: 1300\n",
      "[Train] loss: 1.1513104204529911, score: 0.9375\n",
      "[Dev] loss: 1.142900102967547, score: 0.890625\n",
      "epoch: 17, iteration: 1400\n",
      "[Train] loss: 2.409117607279878, score: 0.78125\n",
      "[Dev] loss: 1.078878744224587, score: 0.903125\n",
      "epoch: 17, iteration: 1500\n",
      "[Train] loss: 1.5820999650980163, score: 0.875\n",
      "[Dev] loss: 1.059180017082206, score: 0.90625\n",
      "epoch: 18, iteration: 0\n",
      "[Train] loss: 2.4046020040863665, score: 0.84375\n",
      "[Dev] loss: 1.13538404250553, score: 0.890625\n",
      "epoch: 18, iteration: 100\n",
      "[Train] loss: 0.8600488937020729, score: 0.90625\n",
      "[Dev] loss: 1.131474692372827, score: 0.89375\n",
      "epoch: 18, iteration: 200\n",
      "[Train] loss: 0.6239003334099738, score: 0.90625\n",
      "[Dev] loss: 1.1243900592940714, score: 0.8875\n",
      "epoch: 18, iteration: 300\n",
      "[Train] loss: 1.6306946939373104, score: 0.875\n",
      "[Dev] loss: 1.0703642045978274, score: 0.903125\n",
      "epoch: 18, iteration: 400\n",
      "[Train] loss: 2.1295348881041267, score: 0.8125\n",
      "[Dev] loss: 1.0620043523614282, score: 0.9\n",
      "epoch: 18, iteration: 500\n",
      "[Train] loss: 0.7791836909089254, score: 0.875\n",
      "[Dev] loss: 1.0688972463400694, score: 0.909375\n",
      "epoch: 18, iteration: 600\n",
      "[Train] loss: 2.2091774497714978, score: 0.78125\n",
      "[Dev] loss: 1.0892914980302206, score: 0.90625\n",
      "epoch: 18, iteration: 700\n",
      "[Train] loss: 0.007605032136974127, score: 1.0\n",
      "[Dev] loss: 1.0661386552794425, score: 0.909375\n",
      "epoch: 18, iteration: 800\n",
      "[Train] loss: 1.5374061420376781, score: 0.84375\n",
      "[Dev] loss: 1.0621311811011023, score: 0.90625\n",
      "epoch: 18, iteration: 900\n",
      "[Train] loss: 2.7697467464433325, score: 0.75\n",
      "[Dev] loss: 1.1022585171820183, score: 0.903125\n",
      "epoch: 18, iteration: 1000\n",
      "[Train] loss: 1.6482832131405218, score: 0.875\n",
      "[Dev] loss: 1.0788127936409777, score: 0.90625\n",
      "epoch: 18, iteration: 1100\n",
      "[Train] loss: 0.8711707767642669, score: 0.90625\n",
      "[Dev] loss: 1.0785622103255914, score: 0.9\n",
      "epoch: 18, iteration: 1200\n",
      "[Train] loss: 0.55654228207443, score: 0.90625\n",
      "[Dev] loss: 1.1106629128834342, score: 0.896875\n",
      "epoch: 18, iteration: 1300\n",
      "[Train] loss: 2.039463967264158, score: 0.84375\n",
      "[Dev] loss: 1.1178637735811354, score: 0.9\n",
      "epoch: 18, iteration: 1400\n",
      "[Train] loss: 1.1891440574695253, score: 0.90625\n",
      "[Dev] loss: 1.0751165331743182, score: 0.9\n",
      "epoch: 18, iteration: 1500\n",
      "[Train] loss: 0.9677404926469279, score: 0.90625\n",
      "[Dev] loss: 1.0851114250854808, score: 0.903125\n",
      "epoch: 19, iteration: 0\n",
      "[Train] loss: 0.6077616611616521, score: 0.9375\n",
      "[Dev] loss: 1.0911845292257136, score: 0.9\n",
      "epoch: 19, iteration: 100\n",
      "[Train] loss: 1.0815673201758644, score: 0.90625\n",
      "[Dev] loss: 1.0814735124583557, score: 0.9\n",
      "epoch: 19, iteration: 200\n",
      "[Train] loss: 0.6889563303605759, score: 0.90625\n",
      "[Dev] loss: 1.0627027220521554, score: 0.896875\n",
      "epoch: 19, iteration: 300\n",
      "[Train] loss: 2.3168497830334402, score: 0.8125\n",
      "[Dev] loss: 1.088327399367648, score: 0.890625\n",
      "epoch: 19, iteration: 400\n",
      "[Train] loss: 0.030434830979155468, score: 1.0\n",
      "[Dev] loss: 1.0521523389864336, score: 0.896875\n",
      "epoch: 19, iteration: 500\n",
      "[Train] loss: 0.4139228347961107, score: 0.9375\n",
      "[Dev] loss: 1.0110671705817271, score: 0.903125\n",
      "epoch: 19, iteration: 600\n",
      "[Train] loss: 0.9836353714279171, score: 0.90625\n",
      "[Dev] loss: 1.060135705253878, score: 0.903125\n",
      "epoch: 19, iteration: 700\n",
      "[Train] loss: 1.982344387936607, score: 0.8125\n",
      "[Dev] loss: 1.0787144228640309, score: 0.9\n",
      "epoch: 19, iteration: 800\n",
      "[Train] loss: 0.3736391164649015, score: 0.96875\n",
      "[Dev] loss: 1.058209186185715, score: 0.903125\n",
      "epoch: 19, iteration: 900\n",
      "[Train] loss: 0.7996696614106674, score: 0.90625\n",
      "[Dev] loss: 1.0447168233831623, score: 0.90625\n",
      "epoch: 19, iteration: 1000\n",
      "[Train] loss: 1.1602892389830477, score: 0.9375\n",
      "[Dev] loss: 1.0662985348769372, score: 0.903125\n",
      "epoch: 19, iteration: 1100\n",
      "[Train] loss: 0.7576778892880347, score: 0.9375\n",
      "[Dev] loss: 1.0821229628123241, score: 0.903125\n",
      "epoch: 19, iteration: 1200\n",
      "[Train] loss: 0.9544619434274583, score: 0.90625\n",
      "[Dev] loss: 1.0754076603018363, score: 0.896875\n",
      "epoch: 19, iteration: 1300\n",
      "[Train] loss: 1.1195092471143318, score: 0.875\n",
      "[Dev] loss: 1.0521489833517481, score: 0.896875\n",
      "epoch: 19, iteration: 1400\n",
      "[Train] loss: 0.7753340919371805, score: 0.9375\n",
      "[Dev] loss: 1.0640222042953087, score: 0.90625\n",
      "epoch: 19, iteration: 1500\n",
      "[Train] loss: 1.98474214413302, score: 0.8125\n",
      "[Dev] loss: 1.0923376128070497, score: 0.903125\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T23:59:44.940905Z",
     "start_time": "2025-04-04T15:50:12.634998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\"\"\"       conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2),  # 输出 [batch, 6, 28, 28]\n",
    "            max_pool(), #6*14*14\n",
    "            conv2D(in_channels=6, out_channels=16, kernel_size=3, padding=0), #16*12*12\n",
    "            max_pool(),\n",
    "\n",
    "            Flatten(),\n",
    "            Linear(in_dim=576,out_dim=10) , # 输出 [batch, 10]\"\"\"\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=5, log_iters=100, save_dir=r'./best_models')"
   ],
   "id": "4100c70a88ad9cb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 13.815510555475644, score: 0.25\n",
      "[Dev] loss: 14.518848156049586, score: 0.203125\n",
      "epoch: 0, iteration: 100\n",
      "[Train] loss: 2.7335743191736093, score: 0.84375\n",
      "[Dev] loss: 3.149103768089488, score: 0.815625\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 3.4113499405048904, score: 0.8125\n",
      "[Dev] loss: 2.1233851736377667, score: 0.875\n",
      "epoch: 0, iteration: 300\n",
      "[Train] loss: 1.1512903955919886, score: 0.9375\n",
      "[Dev] loss: 1.8062419392914826, score: 0.89375\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 1.63712693246421, score: 0.909375\n",
      "epoch: 0, iteration: 500\n",
      "[Train] loss: 2.200807014639997, score: 0.875\n",
      "[Dev] loss: 1.7066910637072752, score: 0.9\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 1.7268418038153426, score: 0.90625\n",
      "[Dev] loss: 1.3307690643823373, score: 0.921875\n",
      "epoch: 0, iteration: 700\n",
      "[Train] loss: 2.029760407407785, score: 0.84375\n",
      "[Dev] loss: 1.6622387308965565, score: 0.90625\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 1.3564847056289047, score: 0.90625\n",
      "[Dev] loss: 1.3075607756537155, score: 0.925\n",
      "epoch: 0, iteration: 900\n",
      "[Train] loss: 1.9119780391333676, score: 0.84375\n",
      "[Dev] loss: 1.3865320188435248, score: 0.909375\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 1.1722467632099365, score: 0.93125\n",
      "epoch: 0, iteration: 1100\n",
      "[Train] loss: -9.999965167000542e-09, score: 1.0\n",
      "[Dev] loss: 1.260999361330569, score: 0.925\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 1.1513055987834402, score: 0.9375\n",
      "[Dev] loss: 1.069422828868022, score: 0.93125\n",
      "epoch: 0, iteration: 1300\n",
      "[Train] loss: 2.874364000168293, score: 0.84375\n",
      "[Dev] loss: 1.0452774071235507, score: 0.940625\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 1.1512925371220217, score: 0.9375\n",
      "[Dev] loss: 0.9567526638635077, score: 0.9375\n",
      "epoch: 0, iteration: 1500\n",
      "[Train] loss: 0.40853092924303963, score: 0.9375\n",
      "[Dev] loss: 1.0521687516118337, score: 0.9375\n",
      "best accuracy performence has been updated: 0.00000 --> 0.90312\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 1.283346231385033, score: 0.90625\n",
      "[Dev] loss: 0.9836607093407356, score: 0.940625\n",
      "epoch: 1, iteration: 100\n",
      "[Train] loss: 1.1578202465522576, score: 0.9375\n",
      "[Dev] loss: 0.8393030239524354, score: 0.95\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 1.7269396729923892, score: 0.90625\n",
      "[Dev] loss: 0.8826700071320343, score: 0.946875\n",
      "epoch: 1, iteration: 300\n",
      "[Train] loss: 2.207850220343726, score: 0.875\n",
      "[Dev] loss: 0.8014529662366641, score: 0.95\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 1.151285531354661, score: 0.9375\n",
      "[Dev] loss: 0.9900880561798437, score: 0.94375\n",
      "epoch: 1, iteration: 500\n",
      "[Train] loss: 0.9742894678843904, score: 0.9375\n",
      "[Dev] loss: 0.9272480849697871, score: 0.946875\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 1.151294223006722, score: 0.9375\n",
      "[Dev] loss: 0.9798124958691306, score: 0.9375\n",
      "epoch: 1, iteration: 700\n",
      "[Train] loss: -9.999999882286399e-09, score: 1.0\n",
      "[Dev] loss: 0.901876984925703, score: 0.95\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 0.39726969213684865, score: 0.96875\n",
      "[Dev] loss: 0.6948701636691725, score: 0.959375\n",
      "epoch: 1, iteration: 900\n",
      "[Train] loss: 2.302588222100545, score: 0.875\n",
      "[Dev] loss: 0.6662232312166323, score: 0.959375\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 1.1512965301829718, score: 0.9375\n",
      "[Dev] loss: 0.6788578993357344, score: 0.95625\n",
      "epoch: 1, iteration: 1100\n",
      "[Train] loss: 1.7240197823283565, score: 0.90625\n",
      "[Dev] loss: 0.7515823837114308, score: 0.95625\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 0.5757138261199133, score: 0.96875\n",
      "[Dev] loss: 0.7849488120990679, score: 0.95\n",
      "epoch: 1, iteration: 1300\n",
      "[Train] loss: 1.1516527307600464, score: 0.9375\n",
      "[Dev] loss: 0.8110798670236499, score: 0.953125\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: 0.976681026773506, score: 0.9375\n",
      "[Dev] loss: 0.6610948146804325, score: 0.95\n",
      "epoch: 1, iteration: 1500\n",
      "[Train] loss: 0.7286861507057069, score: 0.9375\n",
      "[Dev] loss: 0.8820635498163905, score: 0.946875\n",
      "best accuracy performence has been updated: 0.90312 --> 0.94688\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 0.5756468686400347, score: 0.96875\n",
      "[Dev] loss: 0.8180159277970042, score: 0.95\n",
      "epoch: 2, iteration: 100\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 0.8015702426053568, score: 0.94375\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.7381303563507708, score: 0.95625\n",
      "epoch: 2, iteration: 300\n",
      "[Train] loss: 1.160982975448386, score: 0.9375\n",
      "[Dev] loss: 0.7464381821700343, score: 0.95625\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 1.726938809445854, score: 0.90625\n",
      "[Dev] loss: 0.7948219709777439, score: 0.95\n",
      "epoch: 2, iteration: 500\n",
      "[Train] loss: 1.9467702810320173, score: 0.84375\n",
      "[Dev] loss: 0.7102451301144119, score: 0.95625\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 1.1512925651901762, score: 0.9375\n",
      "[Dev] loss: 0.7725778551523375, score: 0.953125\n",
      "epoch: 2, iteration: 700\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.618817284928383, score: 0.9625\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 1.7269390561151883, score: 0.90625\n",
      "[Dev] loss: 0.6117598629109962, score: 0.9625\n",
      "epoch: 2, iteration: 900\n",
      "[Train] loss: 8.153866567116114e-06, score: 1.0\n",
      "[Dev] loss: 0.6519373858415418, score: 0.95625\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 1.1510925253242141, score: 0.9375\n",
      "[Dev] loss: 0.8145583363016341, score: 0.95\n",
      "epoch: 2, iteration: 1100\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 0.8070924015172107, score: 0.953125\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 1.1512925371220242, score: 0.9375\n",
      "[Dev] loss: 0.6652343085012488, score: 0.9625\n",
      "epoch: 2, iteration: 1300\n",
      "[Train] loss: 0.6172224869621048, score: 0.9375\n",
      "[Dev] loss: 0.6235415425060472, score: 0.959375\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: -9.85900894223397e-09, score: 1.0\n",
      "[Dev] loss: 0.5964632404947731, score: 0.95625\n",
      "epoch: 2, iteration: 1500\n",
      "[Train] loss: 0.5756466160500788, score: 0.96875\n",
      "[Dev] loss: 0.6246891075044708, score: 0.953125\n",
      "best accuracy performence has been updated: 0.94688 --> 0.95312\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.829330657479912, score: 0.953125\n",
      "epoch: 3, iteration: 100\n",
      "[Train] loss: 0.5875992064110287, score: 0.96875\n",
      "[Dev] loss: 0.6942224000261961, score: 0.959375\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: -9.999999882286399e-09, score: 1.0\n",
      "[Dev] loss: 0.6926075430118175, score: 0.95625\n",
      "epoch: 3, iteration: 300\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.5237395356833182, score: 0.96875\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 0.3429494208187763, score: 0.96875\n",
      "[Dev] loss: 0.646150650861955, score: 0.959375\n",
      "epoch: 3, iteration: 500\n",
      "[Train] loss: 1.1497663511910412, score: 0.9375\n",
      "[Dev] loss: 0.735894974598343, score: 0.959375\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 0.15349505299075464, score: 0.96875\n",
      "[Dev] loss: 0.712182646866565, score: 0.95625\n",
      "epoch: 3, iteration: 700\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.6986252547141341, score: 0.959375\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.6320802112467616, score: 0.95625\n",
      "epoch: 3, iteration: 900\n",
      "[Train] loss: 9.012172698780902e-08, score: 1.0\n",
      "[Dev] loss: 0.6774010136993411, score: 0.959375\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 1.0917467921957804, score: 0.9375\n",
      "[Dev] loss: 0.7971119041409006, score: 0.953125\n",
      "epoch: 3, iteration: 1100\n",
      "[Train] loss: 0.0663245876057368, score: 0.96875\n",
      "[Dev] loss: 0.6729848774915448, score: 0.95625\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.7280128200388616, score: 0.95\n",
      "epoch: 3, iteration: 1300\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.7084875827591042, score: 0.959375\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.5634763834363504, score: 0.96875\n",
      "epoch: 3, iteration: 1500\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.6618744725625978, score: 0.9625\n",
      "best accuracy performence has been updated: 0.95312 --> 0.96250\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: 5.363964629320508e-05, score: 1.0\n",
      "[Dev] loss: 0.673916652603421, score: 0.9625\n",
      "epoch: 4, iteration: 100\n",
      "[Train] loss: 0.5795199866269213, score: 0.96875\n",
      "[Dev] loss: 0.7113386846857989, score: 0.959375\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 1.3630415288681523, score: 0.90625\n",
      "[Dev] loss: 0.7745356723101569, score: 0.95625\n",
      "epoch: 4, iteration: 300\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.6265396979003086, score: 0.9625\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 1.1519643734215959, score: 0.9375\n",
      "[Dev] loss: 0.6787515832499049, score: 0.959375\n",
      "epoch: 4, iteration: 500\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 0.62342091332165, score: 0.9625\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: 0.0001424318925714077, score: 1.0\n",
      "[Dev] loss: 0.6945007048793508, score: 0.959375\n",
      "epoch: 4, iteration: 700\n",
      "[Train] loss: 1.5011700981653124, score: 0.90625\n",
      "[Dev] loss: 0.6325944767323262, score: 0.965625\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.17133167474279, score: 0.9375\n",
      "[Dev] loss: 0.5928656511346506, score: 0.9625\n",
      "epoch: 4, iteration: 900\n",
      "[Train] loss: -9.999997481429131e-09, score: 1.0\n",
      "[Dev] loss: 0.6629749951428943, score: 0.959375\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: -9.999999049619137e-09, score: 1.0\n",
      "[Dev] loss: 0.5405676256208618, score: 0.965625\n",
      "epoch: 4, iteration: 1100\n",
      "[Train] loss: 0.4630186910663334, score: 0.96875\n",
      "[Dev] loss: 0.610736393781156, score: 0.95625\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: 0.9282663047018318, score: 0.9375\n",
      "[Dev] loss: 0.7109224124247279, score: 0.959375\n",
      "epoch: 4, iteration: 1300\n",
      "[Train] loss: 1.5505005244419368, score: 0.90625\n",
      "[Dev] loss: 0.6683572302302535, score: 0.9625\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.6389540868656673, score: 0.9625\n",
      "epoch: 4, iteration: 1500\n",
      "[Train] loss: 0.2323633363483337, score: 0.96875\n",
      "[Dev] loss: 0.6907690041142944, score: 0.9625\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:08:00.318767Z",
     "start_time": "2025-04-05T15:43:47.175857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=8, log_iters=200, save_dir=r'./best_models')"
   ],
   "id": "bec8d66fc611c21a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 13.815510555475644, score: 0.25\n",
      "[Dev] loss: 14.518848156049586, score: 0.203125\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 3.4113499405048904, score: 0.8125\n",
      "[Dev] loss: 2.1233851736377667, score: 0.875\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 1.63712693246421, score: 0.909375\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 1.7268418038153426, score: 0.90625\n",
      "[Dev] loss: 1.3307690643823373, score: 0.921875\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 1.3564847056289047, score: 0.90625\n",
      "[Dev] loss: 1.3075607756537155, score: 0.925\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 1.1722467632099365, score: 0.93125\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 1.1513055987834402, score: 0.9375\n",
      "[Dev] loss: 1.069422828868022, score: 0.93125\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 1.1512925371220217, score: 0.9375\n",
      "[Dev] loss: 0.9567526638635077, score: 0.9375\n",
      "best accuracy performence has been updated: 0.00000 --> 0.90312\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 1.283346231385033, score: 0.90625\n",
      "[Dev] loss: 0.9836607093407356, score: 0.940625\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 1.7269396729923892, score: 0.90625\n",
      "[Dev] loss: 0.8826700071320343, score: 0.946875\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 1.151285531354661, score: 0.9375\n",
      "[Dev] loss: 0.9900880561798437, score: 0.94375\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 1.151294223006722, score: 0.9375\n",
      "[Dev] loss: 0.9798124958691306, score: 0.9375\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 0.39726969213684865, score: 0.96875\n",
      "[Dev] loss: 0.6948701636691725, score: 0.959375\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 1.1512965301829718, score: 0.9375\n",
      "[Dev] loss: 0.6788578993357344, score: 0.95625\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 0.5757138261199133, score: 0.96875\n",
      "[Dev] loss: 0.7849488120990679, score: 0.95\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: 0.976681026773506, score: 0.9375\n",
      "[Dev] loss: 0.6610948146804325, score: 0.95\n",
      "best accuracy performence has been updated: 0.90312 --> 0.94688\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 0.5756468686400347, score: 0.96875\n",
      "[Dev] loss: 0.8180159277970042, score: 0.95\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.7381303563507708, score: 0.95625\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 1.726938809445854, score: 0.90625\n",
      "[Dev] loss: 0.7948219709777439, score: 0.95\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 1.1512925651901762, score: 0.9375\n",
      "[Dev] loss: 0.7725778551523375, score: 0.953125\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 1.7269390561151883, score: 0.90625\n",
      "[Dev] loss: 0.6117598629109962, score: 0.9625\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 1.1510925253242141, score: 0.9375\n",
      "[Dev] loss: 0.8145583363016341, score: 0.95\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 1.1512925371220242, score: 0.9375\n",
      "[Dev] loss: 0.6652343085012488, score: 0.9625\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: -9.85900894223397e-09, score: 1.0\n",
      "[Dev] loss: 0.5964632404947731, score: 0.95625\n",
      "best accuracy performence has been updated: 0.94688 --> 0.95312\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.829330657479912, score: 0.953125\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: -9.999999882286399e-09, score: 1.0\n",
      "[Dev] loss: 0.6926075430118175, score: 0.95625\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 0.3429494208187763, score: 0.96875\n",
      "[Dev] loss: 0.646150650861955, score: 0.959375\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 0.15349505299075464, score: 0.96875\n",
      "[Dev] loss: 0.712182646866565, score: 0.95625\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 0.5756462635610115, score: 0.96875\n",
      "[Dev] loss: 0.6320802112467616, score: 0.95625\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 1.0917467921957804, score: 0.9375\n",
      "[Dev] loss: 0.7971119041409006, score: 0.953125\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.7280128200388616, score: 0.95\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.5634763834363504, score: 0.96875\n",
      "best accuracy performence has been updated: 0.95312 --> 0.96250\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: 5.363964629320508e-05, score: 1.0\n",
      "[Dev] loss: 0.673916652603421, score: 0.9625\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 1.3630415288681523, score: 0.90625\n",
      "[Dev] loss: 0.7745356723101569, score: 0.95625\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 1.1519643734215959, score: 0.9375\n",
      "[Dev] loss: 0.6787515832499049, score: 0.959375\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: 0.0001424318925714077, score: 1.0\n",
      "[Dev] loss: 0.6945007048793508, score: 0.959375\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.17133167474279, score: 0.9375\n",
      "[Dev] loss: 0.5928656511346506, score: 0.9625\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: -9.999999049619137e-09, score: 1.0\n",
      "[Dev] loss: 0.5405676256208618, score: 0.965625\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: 0.9282663047018318, score: 0.9375\n",
      "[Dev] loss: 0.7109224124247279, score: 0.959375\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: 0.5756462635610116, score: 0.96875\n",
      "[Dev] loss: 0.6389540868656673, score: 0.9625\n",
      "epoch: 5, iteration: 0\n",
      "[Train] loss: 0.5756462635628639, score: 0.96875\n",
      "[Dev] loss: 0.6586702291186578, score: 0.9625\n",
      "epoch: 5, iteration: 200\n",
      "[Train] loss: 0.5756462635610217, score: 0.96875\n",
      "[Dev] loss: 0.6845560516644615, score: 0.9625\n",
      "epoch: 5, iteration: 400\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 0.656637966531122, score: 0.95625\n",
      "epoch: 5, iteration: 600\n",
      "[Train] loss: 1.0646849153090143e-06, score: 1.0\n",
      "[Dev] loss: 0.6730669207225833, score: 0.959375\n",
      "epoch: 5, iteration: 800\n",
      "[Train] loss: 0.8236059277074023, score: 0.9375\n",
      "[Dev] loss: 0.5830391523146619, score: 0.9625\n",
      "epoch: 5, iteration: 1000\n",
      "[Train] loss: 2.2839178938715867e-07, score: 1.0\n",
      "[Dev] loss: 0.6712751847332138, score: 0.95625\n",
      "epoch: 5, iteration: 1200\n",
      "[Train] loss: -9.99998486652014e-09, score: 1.0\n",
      "[Dev] loss: 0.6845708615873579, score: 0.9625\n",
      "epoch: 5, iteration: 1400\n",
      "[Train] loss: 1.0381389396370855, score: 0.9375\n",
      "[Dev] loss: 0.7140645313463143, score: 0.959375\n",
      "epoch: 6, iteration: 0\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.6731643608036301, score: 0.959375\n",
      "epoch: 6, iteration: 200\n",
      "[Train] loss: 0.5761397622342815, score: 0.96875\n",
      "[Dev] loss: 0.49513481435075946, score: 0.971875\n",
      "epoch: 6, iteration: 400\n",
      "[Train] loss: 0.5756446300619333, score: 0.96875\n",
      "[Dev] loss: 0.6154157560842732, score: 0.959375\n",
      "epoch: 6, iteration: 600\n",
      "[Train] loss: 0.9048206714794071, score: 0.9375\n",
      "[Dev] loss: 0.6739045757208195, score: 0.9625\n",
      "epoch: 6, iteration: 800\n",
      "[Train] loss: 0.5765538911630043, score: 0.96875\n",
      "[Dev] loss: 0.629643830793784, score: 0.9625\n",
      "epoch: 6, iteration: 1000\n",
      "[Train] loss: 7.946155725127217e-08, score: 1.0\n",
      "[Dev] loss: 0.690226946985094, score: 0.9625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 11\u001B[0m\n\u001B[0;32m      7\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39mMultiCrossEntropyLoss(model\u001B[38;5;241m=\u001B[39mCNN_model, max_classes\u001B[38;5;241m=\u001B[39mtrain_labs\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      9\u001B[0m runner \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mrunner\u001B[38;5;241m.\u001B[39mRunnerM(CNN_model, optimizer, nn\u001B[38;5;241m.\u001B[39mmetric\u001B[38;5;241m.\u001B[39maccuracy, loss_fn, scheduler\u001B[38;5;241m=\u001B[39mscheduler)\n\u001B[1;32m---> 11\u001B[0m runner\u001B[38;5;241m.\u001B[39mtrain([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, log_iters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, save_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./best_models\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\runner.py:62\u001B[0m, in \u001B[0;36mRunnerM.train\u001B[1;34m(self, train_set, dev_set, **kwargs)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 62\u001B[0m dev_score, dev_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(dev_set)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdev_scores\u001B[38;5;241m.\u001B[39mappend(dev_score)\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdev_loss\u001B[38;5;241m.\u001B[39mappend(dev_loss)\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\runner.py:80\u001B[0m, in \u001B[0;36mRunnerM.evaluate\u001B[1;34m(self, data_set)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_set):\n\u001B[0;32m     79\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m data_set\n\u001B[1;32m---> 80\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(X)\n\u001B[0;32m     81\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(logits, y)\n\u001B[0;32m     82\u001B[0m     score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetric(logits, y)\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\models.py:100\u001B[0m, in \u001B[0;36mModel_CNN.__call__\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m--> 100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(X)\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\models.py:107\u001B[0m, in \u001B[0;36mModel_CNN.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    105\u001B[0m     X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m28\u001B[39m)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 107\u001B[0m     X \u001B[38;5;241m=\u001B[39m layer(X)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\op.py:328\u001B[0m, in \u001B[0;36mmax_pool.__call__\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(X)\n",
      "File \u001B[1;32m~\\Desktop\\神经网络与深度学习\\PJ1\\PJ1-4\\codes\\mynn\\op.py:365\u001B[0m, in \u001B[0;36mmax_pool.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    362\u001B[0m                 output[b, c, i, j] \u001B[38;5;241m=\u001B[39m max_val\n\u001B[0;32m    364\u001B[0m                 \u001B[38;5;66;03m# 记录最大值位置（多个相同最大值时选择第一个）\u001B[39;00m\n\u001B[1;32m--> 365\u001B[0m                 max_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munravel_index(np\u001B[38;5;241m.\u001B[39margmax(window), window\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    366\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask[b, c, h_start \u001B[38;5;241m+\u001B[39m max_idx[\u001B[38;5;241m0\u001B[39m], w_start \u001B[38;5;241m+\u001B[39m max_idx[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36munravel_index\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with dropout:",
   "id": "e724e45b3f7da1f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:08:18.822912Z",
     "start_time": "2025-04-10T15:18:45.228651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\"\"\"       conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2),  # 输出 [batch, 6, 28, 28]\n",
    "            max_pool(), #6*14*14\n",
    "            conv2D(in_channels=6, out_channels=16, kernel_size=3, padding=0), #16*12*12\n",
    "            max_pool(),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dropout(p=0.8),\n",
    "\n",
    "            Linear(in_dim=576,out_dim=10) , # 输出 [batch, 10]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=5, log_iters=200, save_dir=r'./best_models')"
   ],
   "id": "feedd78dd5d6c7e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 13.239863807198955, score: 0.28125\n",
      "[Dev] loss: 14.823771438778067, score: 0.1875\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 4.382827223813916, score: 0.75\n",
      "[Dev] loss: 2.9924818854536213, score: 0.828125\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 1.6266698537423958, score: 0.90625\n",
      "[Dev] loss: 1.8848304215638811, score: 0.878125\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 2.370915676434466, score: 0.84375\n",
      "[Dev] loss: 1.6007954762971894, score: 0.909375\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 1.162307413154779, score: 0.9375\n",
      "[Dev] loss: 1.3896738406832714, score: 0.915625\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 0.5756462635262455, score: 0.96875\n",
      "[Dev] loss: 1.4423090392453766, score: 0.909375\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 1.387283965568245, score: 0.90625\n",
      "[Dev] loss: 1.3034538794132016, score: 0.91875\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 2.302562656111307, score: 0.875\n",
      "[Dev] loss: 1.2487093514633085, score: 0.921875\n",
      "best accuracy performence has been updated: 0.00000 --> 0.90938\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 0.9940031440480888, score: 0.90625\n",
      "[Dev] loss: 1.0310340633885375, score: 0.9375\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 1.5359664700579883, score: 0.90625\n",
      "[Dev] loss: 0.9063024288357211, score: 0.94375\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 2.2964562862875924, score: 0.875\n",
      "[Dev] loss: 1.1542976432410836, score: 0.928125\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 0.5756463401230673, score: 0.96875\n",
      "[Dev] loss: 0.9110529324257406, score: 0.95\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 1.7159498835834225e-05, score: 1.0\n",
      "[Dev] loss: 0.8606300882995861, score: 0.95\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 7.0631694900636635e-06, score: 1.0\n",
      "[Dev] loss: 0.6894759677107652, score: 0.959375\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 0.3222214167777606, score: 0.96875\n",
      "[Dev] loss: 0.7779577830252394, score: 0.95\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.730448837967181, score: 0.95\n",
      "best accuracy performence has been updated: 0.90938 --> 0.95937\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 1.152059419945342, score: 0.9375\n",
      "[Dev] loss: 0.7403446400975678, score: 0.95\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.7940941774904491, score: 0.95\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 0.5756462635618447, score: 0.96875\n",
      "[Dev] loss: 0.6598968544665808, score: 0.95625\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 0.5625527051032103, score: 0.96875\n",
      "[Dev] loss: 0.6875649567969824, score: 0.95625\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 2.0001319774144104, score: 0.875\n",
      "[Dev] loss: 0.811274210655234, score: 0.953125\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 0.7749937202476718, score: 0.9375\n",
      "[Dev] loss: 0.8119330481769443, score: 0.95\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 1.1969973298216188, score: 0.90625\n",
      "[Dev] loss: 0.7603886384889709, score: 0.95625\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: 0.31982062178799076, score: 0.96875\n",
      "[Dev] loss: 0.7034006598005078, score: 0.959375\n",
      "best accuracy performence has been updated: 0.95937 --> 0.96250\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 0.9897317892911386, score: 0.9375\n",
      "[Dev] loss: 0.6964763147251316, score: 0.959375\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: 0.0628323660435059, score: 0.96875\n",
      "[Dev] loss: 0.694352758987984, score: 0.959375\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 0.8682277979624307, score: 0.9375\n",
      "[Dev] loss: 0.7592281145324552, score: 0.95625\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 0.3979364878203674, score: 0.96875\n",
      "[Dev] loss: 0.6933781182991068, score: 0.959375\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 5.257823164121692e-05, score: 1.0\n",
      "[Dev] loss: 0.7640006106334962, score: 0.953125\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 0.03871147903342606, score: 0.96875\n",
      "[Dev] loss: 0.6414138414218374, score: 0.959375\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: 1.0171491378614817e-08, score: 1.0\n",
      "[Dev] loss: 0.6941894742256185, score: 0.959375\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: -9.998485024617774e-09, score: 1.0\n",
      "[Dev] loss: 0.6906881886010106, score: 0.9625\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: -9.999999875347505e-09, score: 1.0\n",
      "[Dev] loss: 0.6918000939157909, score: 0.9625\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 0.040724732247247625, score: 0.96875\n",
      "[Dev] loss: 0.7523870222384217, score: 0.95625\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 1.430959984944863, score: 0.90625\n",
      "[Dev] loss: 0.6906575380577744, score: 0.9625\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: -9.804379599809547e-09, score: 1.0\n",
      "[Dev] loss: 0.7197041891748153, score: 0.953125\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.151292537122023, score: 0.9375\n",
      "[Dev] loss: 0.6607557772174705, score: 0.959375\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: 1.6631815614598533, score: 0.90625\n",
      "[Dev] loss: 0.714621770083705, score: 0.959375\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.6694843892959328, score: 0.9625\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: -9.980414965454645e-09, score: 1.0\n",
      "[Dev] loss: 0.6742535622564007, score: 0.95625\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T00:05:18.116488Z",
     "start_time": "2025-04-11T15:20:13.925168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\"\"\"       conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=2),  # 输出 [batch, 6, 28, 28]\n",
    "            max_pool(), #6*14*14\n",
    "            conv2D(in_channels=6, out_channels=16, kernel_size=3, padding=0), #16*12*12\n",
    "            max_pool(),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dropout(p=0.2),\n",
    "\n",
    "            Linear(in_dim=576,out_dim=10) , # 输出 [batch, 10]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=5, log_iters=200, save_dir=r'./best_models')"
   ],
   "id": "29b58b308d3ef7d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 16.118087565621614, score: 0.125\n",
      "[Dev] loss: 16.303826598164772, score: 0.109375\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 3.8470844986609523, score: 0.75\n",
      "[Dev] loss: 3.3799454423257935, score: 0.80625\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 1.5728545034218684, score: 0.90625\n",
      "[Dev] loss: 2.1498218401344222, score: 0.878125\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 1.7258593913945055, score: 0.90625\n",
      "[Dev] loss: 1.767841090214991, score: 0.89375\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 1.0502885545905838, score: 0.90625\n",
      "[Dev] loss: 1.5140448253593513, score: 0.903125\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 0.5370872853090847, score: 0.96875\n",
      "[Dev] loss: 1.2286199222913283, score: 0.9125\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: 0.5768885656654171, score: 0.96875\n",
      "[Dev] loss: 1.4259499871913708, score: 0.915625\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 1.7275718778755733, score: 0.90625\n",
      "[Dev] loss: 1.199405249855048, score: 0.921875\n",
      "best accuracy performence has been updated: 0.00000 --> 0.90000\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 0.8113841821176909, score: 0.9375\n",
      "[Dev] loss: 1.299034389676962, score: 0.915625\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 1.7269387546111026, score: 0.90625\n",
      "[Dev] loss: 0.8927168195836945, score: 0.94375\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 2.4540008456824394, score: 0.84375\n",
      "[Dev] loss: 1.5624150333624165, score: 0.90625\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 0.5757363526322634, score: 0.96875\n",
      "[Dev] loss: 0.8457646144370404, score: 0.94375\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 3.0582172179017037e-06, score: 1.0\n",
      "[Dev] loss: 0.8444205556748905, score: 0.95\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 0.4798135065441058, score: 0.96875\n",
      "[Dev] loss: 0.7489751623037507, score: 0.953125\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 3.3078938970032965e-08, score: 1.0\n",
      "[Dev] loss: 0.7823039772592868, score: 0.95\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: -9.999998987169092e-09, score: 1.0\n",
      "[Dev] loss: 0.8319382453998821, score: 0.946875\n",
      "best accuracy performence has been updated: 0.90000 --> 0.95000\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 1.151295183860185, score: 0.9375\n",
      "[Dev] loss: 0.8135409201864554, score: 0.946875\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.906062395389052, score: 0.940625\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 0.5756462635609984, score: 0.96875\n",
      "[Dev] loss: 0.7661029898051648, score: 0.946875\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 1.4505688428234452e-06, score: 1.0\n",
      "[Dev] loss: 0.6804666421390825, score: 0.953125\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 2.2724203501724203, score: 0.875\n",
      "[Dev] loss: 0.8388131596147159, score: 0.946875\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 0.5190350018035665, score: 0.9375\n",
      "[Dev] loss: 0.8138672735308973, score: 0.953125\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 1.1659010737152795, score: 0.90625\n",
      "[Dev] loss: 0.7764323207896291, score: 0.95625\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: 0.010029639306731144, score: 1.0\n",
      "[Dev] loss: 0.6651399832660964, score: 0.959375\n",
      "best accuracy performence has been updated: 0.95000 --> 0.95625\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 1.1314817476382921, score: 0.9375\n",
      "[Dev] loss: 0.657699217711355, score: 0.95625\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: 0.5750322723900321, score: 0.96875\n",
      "[Dev] loss: 0.6696197288055701, score: 0.9625\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 0.8430963087659916, score: 0.9375\n",
      "[Dev] loss: 0.7143272519532281, score: 0.95625\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 0.44440080402896265, score: 0.96875\n",
      "[Dev] loss: 0.6924686183374813, score: 0.959375\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 0.0013655997532837161, score: 1.0\n",
      "[Dev] loss: 0.7931967407696132, score: 0.953125\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: 0.5816501375612342, score: 0.96875\n",
      "[Dev] loss: 0.6355895252189849, score: 0.9625\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: 1.136022461706935e-05, score: 1.0\n",
      "[Dev] loss: 0.6612728225901315, score: 0.959375\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: -9.288746464261941e-09, score: 1.0\n",
      "[Dev] loss: 0.6437043043276122, score: 0.9625\n",
      "best accuracy performence has been updated: 0.95625 --> 0.96250\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: -9.999999875347505e-09, score: 1.0\n",
      "[Dev] loss: 0.6373264891138859, score: 0.9625\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 5.809890025503792e-06, score: 1.0\n",
      "[Dev] loss: 0.7136289846634947, score: 0.95625\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 1.4879778343518562, score: 0.90625\n",
      "[Dev] loss: 0.6395739979207216, score: 0.9625\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: -9.924998884479742e-09, score: 1.0\n",
      "[Dev] loss: 0.6697709642785736, score: 0.953125\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.151292537122008, score: 0.9375\n",
      "[Dev] loss: 0.6609052608920882, score: 0.9625\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: 1.7262937772543394, score: 0.90625\n",
      "[Dev] loss: 0.6723355975413391, score: 0.95625\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: -9.999999889225291e-09, score: 1.0\n",
      "[Dev] loss: 0.632966739892353, score: 0.965625\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: -9.980120742478311e-09, score: 1.0\n",
      "[Dev] loss: 0.6507704255237267, score: 0.959375\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T15:30:13.677175Z",
     "start_time": "2025-04-12T15:30:08.997759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "np.random.seed(309)\n",
    "\n",
    "train_images_path = r'.\\dataset\\MNIST\\train-images-idx3-ubyte.gz'\n",
    "train_labels_path = r'.\\dataset\\MNIST\\train-labels-idx1-ubyte.gz'\n",
    "\n",
    "# 加载图像数据\n",
    "with gzip.open(train_images_path, 'rb') as f:\n",
    "    magic, num, rows, cols = unpack('>4I', f.read(16))\n",
    "    train_imgs = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows, cols)  # shape (60000, 28, 28)\n",
    "    train_imgs = train_imgs[:, np.newaxis, :, :]  # 最终shape (60000, 1, 28, 28)\n",
    "\n",
    "# 加载标签数据\n",
    "with gzip.open(train_labels_path, 'rb') as f:\n",
    "    magic, num = unpack('>2I', f.read(8))\n",
    "    train_labs = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "# 数据划分\n",
    "idx = np.random.permutation(np.arange(num))\n",
    "train_imgs = train_imgs[idx]\n",
    "train_labs = train_labs[idx]\n",
    "valid_imgs = train_imgs[:320]\n",
    "valid_labs = train_labs[:320]\n",
    "train_imgs = train_imgs[10000:]\n",
    "train_labs = train_labs[10000:]\n",
    "\n",
    "# 归一化\n",
    "train_imgs = train_imgs / 255.0  \n",
    "valid_imgs = valid_imgs / 255.0\n",
    "\n",
    "\n",
    "print(\"\\n=== CNN数据预处理验证 ===\")\n",
    "print(f\"训练集图像形状: {train_imgs.shape} (样本数, 通道数, 高, 宽)\")\n",
    "print(f\"单个样本形状: {train_imgs[0].shape}\")\n",
    "print(f\"训练集标签形状: {train_labs.shape}\")\n",
    "print(f\"验证集图像形状: {valid_imgs.shape}\")\n",
    "print(f\"像素值范围: [{train_imgs.min()}, {train_imgs.max()}]\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.imshow(train_imgs[0][0], cmap='gray')  \n",
    "plt.title(f\"Label: {train_labs[0]}\")\n",
    "plt.axis('off')  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 定义数据增强的变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.01, 0.01), scale=(0.9, 1.1), shear=5),  # 随机仿射变换\n",
    "    transforms.RandomRotation(degrees=5),  # 随机旋转\n",
    "    transforms.RandomResizedCrop(size=(28, 28), scale=(0.9, 1.0)),  # 随机裁剪\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "sample_img = train_imgs[0][0]  \n",
    "\n",
    "# 转换为PIL图像来使用 torchvision.transforms\n",
    "from PIL import Image\n",
    "sample_img_pil = Image.fromarray((sample_img * 255).astype(np.uint8))\n",
    "\n",
    "# 应用数据增强变换\n",
    "augmented_img = transform(sample_img_pil)\n",
    "\n",
    "# 可视化增强后的图像\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.imshow(augmented_img.squeeze(), cmap='gray')  \n",
    "plt.title(f\"Augmented Label: {train_labs[0]}\")\n",
    "plt.axis('off')  \n",
    "plt.show()"
   ],
   "id": "a8756c70413a6dd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CNN数据预处理验证 ===\n",
      "训练集图像形状: (50000, 1, 28, 28) (样本数, 通道数, 高, 宽)\n",
      "单个样本形状: (1, 28, 28)\n",
      "训练集标签形状: (50000,)\n",
      "验证集图像形状: (320, 1, 28, 28)\n",
      "像素值范围: [0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:08:56.430633Z",
     "start_time": "2025-04-09T15:08:56.423550Z"
    }
   },
   "cell_type": "code",
   "source": "print(augmented_img.shape)",
   "id": "96c351b3b5b10f52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T15:30:26.969220Z",
     "start_time": "2025-04-12T15:30:16.482472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "augmented_train_imgs = []\n",
    "\n",
    "for img in train_imgs:\n",
    "    # 转换为PIL图像以使用 torchvision 的 transforms\n",
    "    img_pil = Image.fromarray((img[0] * 255).astype(np.uint8))  # train_imgs 是 [batch, 1, 28, 28]\n",
    "    \n",
    "    augmented_img = transform(img_pil)\n",
    "    \n",
    "    \n",
    "    augmented_train_imgs.append(augmented_img.numpy())  \n",
    "\n",
    "\n",
    "augmented_train_imgs = np.array(augmented_train_imgs)\n",
    "\n",
    "# 查看增强后的前几个图像\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    ax.imshow(augmented_train_imgs[i][0], cmap='gray')  \n",
    "    ax.set_title(f\"Label: {train_labs[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ],
   "id": "532515b43255c35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:09:10.834952Z",
     "start_time": "2025-04-09T15:09:10.831173Z"
    }
   },
   "cell_type": "code",
   "source": "print(augmented_train_imgs.shape)",
   "id": "3fbac66542befcb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T00:15:04.807742Z",
     "start_time": "2025-04-12T15:30:29.217262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CNN_model=nn.models.Model_CNN()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = nn.optimizer.SGD(init_lr=0.03, model=CNN_model)\n",
    "scheduler = nn.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2000, 4000, 8000], gamma=0.5)\n",
    "loss_fn = nn.op.MultiCrossEntropyLoss(model=CNN_model, max_classes=train_labs.max()+1)\n",
    "\n",
    "runner = nn.runner.RunnerM(CNN_model, optimizer, nn.metric.accuracy, loss_fn, scheduler=scheduler)\n",
    "\n",
    "runner.train([augmented_train_imgs, train_labs], [valid_imgs, valid_labs], num_epochs=5, log_iters=200, save_dir=r'./best_models')"
   ],
   "id": "3d29d84db3fae403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0\n",
      "[Train] loss: 15.54244937614731, score: 0.15625\n",
      "[Dev] loss: 16.005892328112743, score: 0.125\n",
      "epoch: 0, iteration: 200\n",
      "[Train] loss: 7.259434133319919, score: 0.59375\n",
      "[Dev] loss: 2.926403623087519, score: 0.83125\n",
      "epoch: 0, iteration: 400\n",
      "[Train] loss: 3.0060861693158114, score: 0.8125\n",
      "[Dev] loss: 2.0591788444388275, score: 0.86875\n",
      "epoch: 0, iteration: 600\n",
      "[Train] loss: 1.7344992437654847, score: 0.90625\n",
      "[Dev] loss: 1.9710259702497652, score: 0.8875\n",
      "epoch: 0, iteration: 800\n",
      "[Train] loss: 2.1072857493292023, score: 0.875\n",
      "[Dev] loss: 2.0197595388714213, score: 0.884375\n",
      "epoch: 0, iteration: 1000\n",
      "[Train] loss: 2.0031870754582894, score: 0.875\n",
      "[Dev] loss: 1.9048209226698596, score: 0.884375\n",
      "epoch: 0, iteration: 1200\n",
      "[Train] loss: -9.931154231497047e-09, score: 1.0\n",
      "[Dev] loss: 1.311161761145183, score: 0.91875\n",
      "epoch: 0, iteration: 1400\n",
      "[Train] loss: 1.726938810690248, score: 0.90625\n",
      "[Dev] loss: 1.317293189344301, score: 0.915625\n",
      "best accuracy performence has been updated: 0.00000 --> 0.88750\n",
      "epoch: 1, iteration: 0\n",
      "[Train] loss: 3.690595860691583, score: 0.78125\n",
      "[Dev] loss: 1.5535959948203109, score: 0.89375\n",
      "epoch: 1, iteration: 200\n",
      "[Train] loss: 0.5547562733485408, score: 0.96875\n",
      "[Dev] loss: 1.5344897869559915, score: 0.909375\n",
      "epoch: 1, iteration: 400\n",
      "[Train] loss: 1.8950354007612622, score: 0.875\n",
      "[Dev] loss: 1.2705583870951187, score: 0.928125\n",
      "epoch: 1, iteration: 600\n",
      "[Train] loss: 1.151300447735326, score: 0.9375\n",
      "[Dev] loss: 1.2495373027582815, score: 0.91875\n",
      "epoch: 1, iteration: 800\n",
      "[Train] loss: 1.4204433413439947, score: 0.875\n",
      "[Dev] loss: 1.0154956350253936, score: 0.928125\n",
      "epoch: 1, iteration: 1000\n",
      "[Train] loss: 0.5487397786298099, score: 0.96875\n",
      "[Dev] loss: 0.9828181507657977, score: 0.928125\n",
      "epoch: 1, iteration: 1200\n",
      "[Train] loss: 1.7264575889298528, score: 0.90625\n",
      "[Dev] loss: 0.8981737110542454, score: 0.9375\n",
      "epoch: 1, iteration: 1400\n",
      "[Train] loss: 0.47541385613321524, score: 0.96875\n",
      "[Dev] loss: 0.9906904040641766, score: 0.934375\n",
      "best accuracy performence has been updated: 0.88750 --> 0.94688\n",
      "epoch: 2, iteration: 0\n",
      "[Train] loss: 0.01332136240561639, score: 1.0\n",
      "[Dev] loss: 0.918904279262855, score: 0.946875\n",
      "epoch: 2, iteration: 200\n",
      "[Train] loss: 0.8443967878994609, score: 0.9375\n",
      "[Dev] loss: 0.8697322404408926, score: 0.94375\n",
      "epoch: 2, iteration: 400\n",
      "[Train] loss: 0.8697013388611339, score: 0.90625\n",
      "[Dev] loss: 0.9264885179198054, score: 0.94375\n",
      "epoch: 2, iteration: 600\n",
      "[Train] loss: 1.4885744894871764, score: 0.90625\n",
      "[Dev] loss: 0.8213808723146766, score: 0.9375\n",
      "epoch: 2, iteration: 800\n",
      "[Train] loss: 1.2953762878860466, score: 0.90625\n",
      "[Dev] loss: 0.8465188342774794, score: 0.94375\n",
      "epoch: 2, iteration: 1000\n",
      "[Train] loss: 0.793871979737811, score: 0.9375\n",
      "[Dev] loss: 0.932727250438251, score: 0.946875\n",
      "epoch: 2, iteration: 1200\n",
      "[Train] loss: 0.5977392449843859, score: 0.9375\n",
      "[Dev] loss: 0.8155071402911884, score: 0.95\n",
      "epoch: 2, iteration: 1400\n",
      "[Train] loss: 0.0025409249490924834, score: 1.0\n",
      "[Dev] loss: 0.9422843340102455, score: 0.94375\n",
      "best accuracy performence has been updated: 0.94688 --> 0.95312\n",
      "epoch: 3, iteration: 0\n",
      "[Train] loss: 1.7468035517861304, score: 0.875\n",
      "[Dev] loss: 0.7998417955682782, score: 0.95\n",
      "epoch: 3, iteration: 200\n",
      "[Train] loss: 1.7637527506503146, score: 0.875\n",
      "[Dev] loss: 0.8786645919122501, score: 0.94375\n",
      "epoch: 3, iteration: 400\n",
      "[Train] loss: 1.1512960509612022, score: 0.9375\n",
      "[Dev] loss: 0.8869723396267034, score: 0.94375\n",
      "epoch: 3, iteration: 600\n",
      "[Train] loss: 0.5758780827927505, score: 0.96875\n",
      "[Dev] loss: 0.7936311528443116, score: 0.95\n",
      "epoch: 3, iteration: 800\n",
      "[Train] loss: 1.7305213434991475, score: 0.90625\n",
      "[Dev] loss: 0.7875650894995736, score: 0.94375\n",
      "epoch: 3, iteration: 1000\n",
      "[Train] loss: -6.566038002220611e-09, score: 1.0\n",
      "[Dev] loss: 0.8503698147244354, score: 0.946875\n",
      "epoch: 3, iteration: 1200\n",
      "[Train] loss: 1.3211515378326095, score: 0.90625\n",
      "[Dev] loss: 0.7727716952002915, score: 0.95625\n",
      "epoch: 3, iteration: 1400\n",
      "[Train] loss: 1.1512509358400957, score: 0.9375\n",
      "[Dev] loss: 0.804775072660511, score: 0.94375\n",
      "epoch: 4, iteration: 0\n",
      "[Train] loss: 2.396189881444749e-05, score: 1.0\n",
      "[Dev] loss: 0.8239888043988826, score: 0.946875\n",
      "epoch: 4, iteration: 200\n",
      "[Train] loss: 1.082807506532368, score: 0.90625\n",
      "[Dev] loss: 0.8660790901798985, score: 0.95\n",
      "epoch: 4, iteration: 400\n",
      "[Train] loss: 0.5788398309265987, score: 0.96875\n",
      "[Dev] loss: 0.8043707660067462, score: 0.946875\n",
      "epoch: 4, iteration: 600\n",
      "[Train] loss: 2.9039058179668647e-08, score: 1.0\n",
      "[Dev] loss: 0.8529628227018462, score: 0.953125\n",
      "epoch: 4, iteration: 800\n",
      "[Train] loss: 1.1510408538392354, score: 0.9375\n",
      "[Dev] loss: 0.7586282957466044, score: 0.946875\n",
      "epoch: 4, iteration: 1000\n",
      "[Train] loss: 2.32417308490612, score: 0.875\n",
      "[Dev] loss: 0.8001300058403021, score: 0.95\n",
      "epoch: 4, iteration: 1200\n",
      "[Train] loss: 0.5756462242480291, score: 0.96875\n",
      "[Dev] loss: 0.7808167888732853, score: 0.953125\n",
      "epoch: 4, iteration: 1400\n",
      "[Train] loss: 4.835911944147026e-07, score: 1.0\n",
      "[Dev] loss: 0.7560450353668291, score: 0.953125\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
